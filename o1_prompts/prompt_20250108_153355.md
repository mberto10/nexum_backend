<filepath>prompt_builder.md</filepath>

<user_instructions>

I need your help to brainstorm a software design for a financial agent application.
Specifically, i need you to critique my current design and make some recommendations for improvements.

Here are my thoughts: 

- I want to build a financial agent that can do a lot of diverse financial analysis tasks.
- The main data source will be indexed 10k financial documents.
- Current idea for the financial agent is to have the agent first plan out the analysis it needs to do in @analysis_module_plan.py
- Then, i thought about using an orchestrator file to execute the information gathering from the plan. A first draft for the orchestrator is @analysis_module_orchestrator.py
- The orchestrator will have several tools/sub-graphs that it can call, for example @agent_retrieval.py for vector search of the 10k documents (another not yet implemented will be for web search).
- So the information gathering orchestrator will assemble the graph according to the plan it will receive, then call the sub-graphs with the right information in the right order, and in the end evaluate if the information gathered is enough to answer the user query (with a potential re-finement cycle if needed - will be implemented later).
- You do not have to comment on the fact that the code is not yet perfectly working with each other, i will focus on the modular parts first.

Please analyze the design plan and tell me if this is a good design for the information gathering part of the financial agent and what set up would be good for the execution part where we will have an LLM with code interpreter capabilities do the execution based on the plan and the information gathered. 

</user_instructions>



---

<filepath>analysis_module_plan.py</filepath>

import os
import google.generativeai as genai
from typing import TypedDict, Optional, List, Dict, Any
from langgraph.graph.state import StateGraph, START, END
from langchain_core.runnables import RunnableConfig
from langgraph.types import Command
import json
from datetime import datetime

# ----------------------------------------------------------------------------
# 1. State Definition
# ----------------------------------------------------------------------------

class AnalysisState(TypedDict):
    """
    Holds the entire state for the initial user input + planning module.
    """

    user_query: str
    company_name: str
    analysis_type: Optional[str]
    information_needs: Optional[List[Dict[str, str]]]
    execution_plan: Optional[List[Dict[str, Any]]]  
    plan_raw: Optional[Dict[str, Any]]

# ----------------------------------------------------------------------------
# 2. Context Cache (Workflow + Modules Overview)
# ----------------------------------------------------------------------------

CONTEXT_CACHE = """
## TASKCONTEXT
You are an expert financial analysis agent with deep expertise in finance, accounting, financial modeling, financial statements analysis and financial documentation and document structures.
You are a deliberate and comprehensive structured thinker and planner for comprehensive financial analysis.

## TASK
Your task is to plan a comprehensive analysis based on a user's query about a company's financial data and performance. 
You will create a structured plan that outlines the information gathering needs and subsequent analysis steps.
The plan must be thorough, logically sequenced, and follow all specified constraints.

## WORKFLOW CONTEXT
The analysis process is divided into distinct modules:
1. User Input Processing (Current)
2. Analysis Planning (Current - Your Focus)
3. Information Gathering (Separate Module)
4. Analysis Execution (Separate Module)

## BACKGROUND: DOCUMENT STR
The 10-K filing follows a standardized structure with specific items containing distinct types of information:

### Core Business Information
- **Item 1. Business**: Company operations, products/services, revenue streams, competitive landscape, market share, industry trends, key customers, suppliers, seasonality, regulations, IP, R&D, geographic markets, corporate structure
- **Item 1A. Risk Factors**: Major business risks including economic, market, operational, financial, regulatory, cybersecurity, environmental, competition, litigation risks
- **Item 1B. Unresolved Staff Comments**: SEC staff comments on previous filings

### Assets and Legal
- **Item 2. Properties**: Physical assets, facilities, real estate, manufacturing locations, distribution centers
- **Item 3. Legal Proceedings**: Ongoing litigation, regulatory proceedings, patent disputes, settlements
- **Item 4. Mine Safety Disclosures**: Mine safety information if applicable

### Financial Performance
- **Item 5. Market for Registrant's Common Equity**: Stock information, dividends, repurchase programs
- **Item 6. [Reserved]**: Currently reserved by SEC
- **Item 7. Management's Discussion and Analysis**: Business overview, performance indicators, liquidity, operations results, future outlook, trends
- **Item 7A. Market Risk Disclosures**: Analysis of market risks (interest rate, currency, commodity price risks)
- **Item 8. Financial Statements**: Complete audited financial statements, footnotes, accounting policies
- **Item 9. Changes in and Disagreements with Accountants**: Accounting changes and disputes
- **Item 9A. Controls and Procedures**: Internal control assessment
- **Item 9B. Other Information**: Additional material information
- **Item 9C. Foreign Jurisdictions Disclosure**: Information about operations in jurisdictions preventing PCAOB inspections

### Governance and Management
- **Item 10. Directors and Executive Officers**: Leadership details, governance policies
- **Item 11. Executive Compensation**: Detailed compensation information
- **Item 12. Security Ownership**: Major shareholders information
- **Item 13. Related Transactions**: Related party transactions
- **Item 14. Principal Accountant Fees**: Accounting services and fees
- **Item 15. Exhibits**: Additional documentation and certifications
- **Item 16. Form 10-K Summary**: Optional report summary

### Common Analysis Patterns

Here are some common analysis planning patterns that you can adapt to the specific user query for formulating your plan.

1. **Revenue Analysis Pattern**
   - **Core Information Needs**:
     - Historical revenue data by segment/product line
     - Revenue growth rates and trends
     - Geographic distribution of revenue
     - Revenue concentration (key customers/products)
   - **Supporting Information Needs**:
     - Market size and share data
     - Pricing strategies and changes
     - Seasonality patterns
     - Currency exposure impacts
   - **Analysis Components**:
     - Segment contribution analysis
     - Year-over-year growth decomposition
     - Revenue quality assessment
     - Forward-looking indicators

2. **Risk Assessment Pattern**
   - **Core Information Needs**:
     - Identified risk factors and their potential impact
     - Risk mitigation strategies
     - Historical risk events and outcomes
     - Industry-specific risk factors
   - **Supporting Information Needs**:
     - Control measures and their effectiveness
     - Insurance and hedging strategies
     - Regulatory compliance status
     - Pending litigation or disputes
   - **Analysis Components**:
     - Risk severity and likelihood matrix
     - Year-over-year risk evolution
     - Comparative industry risk assessment
     - Risk response effectiveness

3. **Growth Analysis Pattern**
   - **Core Information Needs**:
     - Strategic growth initiatives
     - Market expansion plans
     - R&D investments and pipeline
     - Capital allocation strategy
   - **Supporting Information Needs**:
     - Competitive landscape changes
     - Market penetration metrics
     - Historical execution success
     - Resource availability
   - **Analysis Components**:
     - Growth driver identification
     - Investment return analysis
     - Market opportunity sizing
     - Execution capability assessment

4. **Competitive Position Pattern**
   - **Core Information Needs**:
     - Market share data and trends
     - Competitive advantages
     - Product/service differentiation
     - Cost position and efficiency metrics
   - **Supporting Information Needs**:
     - Industry structure and dynamics
     - Technology and innovation position
     - Brand strength indicators
     - Customer relationship metrics
   - **Analysis Components**:
     - Competitive advantage sustainability
     - Market position trajectory
     - Capability gap analysis
     - Strategic response assessment

5. **Financial Health Pattern**
   - **Core Information Needs**:
     - Key financial metrics and ratios
     - Cash flow composition
     - Capital structure
     - Working capital efficiency
   - **Supporting Information Needs**:
     - Credit ratings and debt covenants
     - Off-balance sheet obligations
     - Liquidity sources
     - Asset quality indicators
   - **Analysis Components**:
     - Financial flexibility assessment
     - Stress test scenarios
     - Capital efficiency analysis
     - Sustainability of financial position

6. **Operational Efficiency Pattern**
   - **Core Information Needs**:
     - Operating margins and trends
     - Cost structure breakdown
     - Productivity metrics
     - Capacity utilization
   - **Supporting Information Needs**:
     - Supply chain performance
     - Quality metrics
     - Employee productivity
     - Asset performance data
   - **Analysis Components**:
     - Margin driver analysis
     - Operational bottleneck identification
     - Efficiency improvement potential
     - Cost optimization opportunities

### Key Information Interconnections
1. **Operational Performance Analysis**
   - Item 7 (MD&A) → Item 8 (Financial Statements)
   - Item 1 (Business Operations) → Item 2 (Operational Assets)
   - Item 1A (Risks) → Item 7 (Risk Mitigation Strategies)

2. **Strategic Analysis**
   - Item 1 (Strategy) → Item 7 (Implementation Progress)
   - Item 1A (Risks) → Item 9A (Control Measures)
   - Item 2 (Assets) → Item 8 (Asset Utilization)

3. **Financial Health Assessment**
   - Item 8 (Financials) → Item 7 (Management's Interpretation)
   - Item 7A (Risks) → Item 7 (Risk Management)
   - Item 1A (Risks) → Item 5 (Market Impact)

### Information Depth Guide
1. **Quantitative Data (High Precision)**
   - Item 8: Detailed financial statements, exact figures
   - Item 5: Specific share counts, prices, dividends
   - Item 11: Precise compensation figures

2. **Qualitative Analysis (Medium-High Detail)**
   - Item 7: Detailed management discussion, trends
   - Item 1: Comprehensive business description
   - Item 1A: Extensive risk descriptions

3. **Contextual Information (Medium Detail)**
   - Item 2: Asset descriptions and locations
   - Item 10: Management backgrounds
   - Item 13: Related party descriptions

4. **Supporting Information (Variable Detail)**
   - Item 9B: Supplementary information
   - Item 15: Referenced documents
   - Item 16: Optional summary

## DATA SOURCES
Primary Source: 10-K Financial Documents
- Stored in Pinecone vector database
- Documents are chunked with metadata tags
- Tags include: top-level item headings, subheadings
- Contains both numerical data and textual analysis

## PLANNING INSTRUCTIONS
Follow these steps to create a comprehensive analysis plan:

1. Query Analysis
   - Deeply understand the user's request
   - Identify key analysis components needed
   - Determine if request needs factual data, qualitative analysis, or both

2. Information Requirements
   - List all required data points and information
   - Distinguish between factual data and broad analysis needs

3. Analysis Structure
   - Break down analysis into logical steps
   - Ensure steps build upon each other
   - Include data processing, calculations, and interpretations

4. Output Planning
   - Define how results should be presented
   - Specify any visualizations or comparisons needed
   - Plan clear and actionable conclusions

## JSON OUTPUT SCHEMA
{
  "analysisType": "string - Categorize the type of analysis (e.g., 'Revenue Analysis', 'Growth Assessment')",
  "informationNeeds": [
    {
      "summary": "Brief description of needed information",
      "description": "Detailed explanation of data format and context needed",
      "tag": "factual | broad",
      "stored": "unique_identifier_for_retrieval"
    }
  ],
  "executionPlan": [
    {
      "stepName": "Clear, descriptive name of the analysis step",
      "actions": [
        {
          "actionType": "FinancialCalculation | TextAnalysis | ComparisonAnalysis | ScenarioModeling | WebResearch | ResultFormatting",
          "task": "Specific instructions for this action",
          "information": ["stored_identifiers_from_information_needs"]
        }
      ]
    }
  ]
}

## CONSTRAINTS
1. Return ONLY valid JSON - no additional text or markdown
2. Final step MUST be ResultFormatting type
3. Do not include specific retrieval queries
4. Ensure all stored identifiers are unique
5. All steps must be logically connected

## QUALITY CHECKS
- Are all required data points identified?
- Is the execution plan logically sequenced?
- Are dependencies between steps clear?
- Is the final output format well-defined?
- Have potential challenges been noted?
"""

# ----------------------------------------------------------------------------
# 3. Configure Gemini & (Optional) Context Caching
# ----------------------------------------------------------------------------

API_KEY = "AIzaSyAp3nWo75vwNQ_e0aUGNDo2qwK_HGKtliI"  # Replace with your actual Gemini API key
genai.configure(api_key=API_KEY)

def create_context_cache():
    """
    Placeholder for advanced caching usage if you want to store large doc(s)
    in a context-based approach. Not implementing in detail here.
    """
    return None

def get_planning_model():
    model = genai.GenerativeModel(
        model_name="gemini-1.5-flash",
        generation_config=genai.types.GenerationConfig(
            temperature=0.2,
            max_output_tokens=2500,
        )
    )

    # Define the response schema for structured output
    model.response_schema = {
        "type": "object",
        "properties": {
            "analysisType": {"type": "string"},
            "informationNeeds": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "summary": {"type": "string"},
                        "description": {"type": "string"},
                        "tag": {
                            "type": "string",
                            "enum": ["factual", "broad"]
                        },
                        "stored": {"type": "string"}
                    },
                    "required": ["summary", "description", "tag", "stored"]
                }
            },
            "executionPlan": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "stepName": {"type": "string"},
                        "actions": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "actionType": {
                                        "type": "string",
                                        "enum": [
                                            "FinancialCalculation",
                                            "TextAnalysis",
                                            "ComparisonAnalysis",
                                            "ScenarioModeling",
                                            "WebResearch",
                                            "ResultFormatting"
                                        ]
                                    },
                                    "task": {"type": "string"},
                                    "information": {
                                        "type": "array",
                                        "items": {"type": "string"}
                                    }
                                },
                                "required": ["actionType", "task", "information"]
                            }
                        }
                    },
                    "required": ["stepName", "actions"]
                }
            }
        },
        "required": [
            "analysisType",
            "informationNeeds",
            "executionPlan"
        ]
    }
    
    return model

# ----------------------------------------------------------------------------
# 4. Planning Node
# ----------------------------------------------------------------------------

def planning_node(state: AnalysisState, config: RunnableConfig, use_context_cache: bool = True) -> AnalysisState:
    """
    Module 2 - Planning:
    1. Accept user_query & company_name from state
    2. Optionally provide the big context (CONTEXT_CACHE) to the LLM
    3. Call Gemini to produce a JSON-based plan
    4. Parse & store each portion into state
    """
    user_query = state.get("user_query", "")
    company_name = state.get("company_name", "")
    if not user_query or not company_name:
        state["plan_raw"] = {
            "error": "Missing user_query or company_name",
            "raw_response": None
        }
        return state

    # Build prompt
    context_part = CONTEXT_CACHE if use_context_cache else ""
    prompt = f"""
{context_part}

User's query: '{user_query}'
Company name: '{company_name}'

Return a JSON object following the schema provided. Do not include any markdown formatting or extra text.
"""

    model = get_planning_model()

    raw_text = ""
    try:
        response = model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=0.0,
                max_output_tokens=1500,
            )
        )
        raw_text = response.text.strip()
        
        # Remove any markdown formatting if present
        if raw_text.startswith("```"):
            raw_text = raw_text.split("```")[1]
            # If there's a "json" fence, remove it
            if raw_text.startswith("json"):
                raw_text = raw_text[4:]
        raw_text = raw_text.strip()
        
        # Parse JSON
        parsed = json.loads(raw_text)
        
        # Store the full plan in plan_raw
        state["plan_raw"] = parsed

        # Distribute fields into separate state variables
        state["analysis_type"] = parsed.get("analysisType")
        state["information_needs"] = parsed.get("informationNeeds")
        state["execution_plan"] = parsed.get("executionPlan")

    except Exception as e:
        state["plan_raw"] = {
            "error": f"Error processing response: {str(e)}",
            "raw_response": raw_text
        }

    return state

# ----------------------------------------------------------------------------
# 5. Graph Setup
# ----------------------------------------------------------------------------

def build_initial_planning_graph(use_context_cache: bool = True):
    """
    Builds a simple LangGraph workflow:
      START -> planning_node -> END
    """
    builder = StateGraph(AnalysisState)

    def planning_node_wrapper(state: AnalysisState, config: RunnableConfig) -> AnalysisState:
        return planning_node(state, config, use_context_cache=use_context_cache)

    builder.add_node("planning_node", planning_node_wrapper)

    builder.add_edge(START, "planning_node")
    builder.add_edge("planning_node", END)

    return builder.compile()

def run_planning_workflow(user_query: str, company_name: str, use_context_cache: bool = True) -> dict:
    """
    High-level function that sets up the graph state with user inputs,
    then runs the planning node.
    """
    graph = build_initial_planning_graph(use_context_cache=use_context_cache)
    initial_state: AnalysisState = {
        "user_query": user_query,
        "company_name": company_name,
        "analysis_type": None,
        "information_needs": None,
        "execution_plan": None,
        "plan_raw": None
    }
    final_state = graph.invoke(initial_state)
    return final_state

def save_plan_as_markdown(state: AnalysisState, output_file: str = None) -> str:
    """
    Save the analysis plan as a well-formatted markdown file.
    Returns the path to the saved file.
    """
    # Create output directory if it doesn't exist
    output_dir = "analysis_plans"
    os.makedirs(output_dir, exist_ok=True)

    # Generate default output name if none provided
    if not output_file:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        company_name = state.get("company_name", "unknown").replace(" ", "_")
        output_file = os.path.join(output_dir, f"analysis_plan_{company_name}_{timestamp}.md")

    # Format the content
    content = []
    content.append(f"# Analysis Plan for {state['company_name']}\n")
    content.append(f"## Query\n{state['user_query']}\n")
    
    if state.get("analysis_type"):
        content.append(f"## Analysis Type\n{state['analysis_type']}\n")
    
    if state.get("information_needs"):
        content.append("## Information Needs\n")
        for i, need in enumerate(state["information_needs"], 1):
            content.append(f"### {i}. {need['summary']}")
            content.append(f"- **Description**: {need['description']}")
            content.append(f"- **Type**: {need['tag']}")
            content.append(f"- **Storage ID**: `{need['stored']}`\n")
    
    if state.get("execution_plan"):
        content.append("## Execution Plan\n")
        for i, step in enumerate(state["execution_plan"], 1):
            content.append(f"### Step {i}: {step['stepName']}")
            for j, action in enumerate(step["actions"], 1):
                content.append(f"#### Action {j}")
                content.append(f"- **Type**: {action['actionType']}")
                content.append(f"- **Task**: {action['task']}")
                if action.get("information"):
                    content.append("- **Required Information**:")
                    for info in action["information"]:
                        content.append(f"  - `{info}`")
                content.append("")

    # Write to file
    with open(output_file, "w", encoding="utf-8") as f:
        f.write("\n".join(content))

    return output_file

# ----------------------------------------------------------------------------
# 6. Testing / Main
# ----------------------------------------------------------------------------

if __name__ == "__main__":
    test_query = "Analyze Amazon's revenue distribution across regions and business segments."
    test_company = "Amazon"
    result = run_planning_workflow(test_query, test_company, use_context_cache=True)
    print("\n===== Analysis Planning Output =====")
    print(f"\nQuery: {test_query}")
    print(f"Company: {test_company}\n")
    
    # Save as markdown
    output_file = save_plan_as_markdown(result)
    print(f"\nAnalysis plan saved to: {output_file}")
    
    # Also print JSON for debugging
    plan = result.get("plan_raw", {})
    formatted_json = json.dumps(plan, indent=2)
    print("\nRaw JSON output:")
    print(formatted_json)

---

<filepath>analysis_module_infogathering_orchestrator.py</filepath>

import os
from typing import TypedDict, Any, List, Dict
from langgraph.graph.state import StateGraph, START, END
from langchain_core.runnables import RunnableConfig

# We'll define a simple state type for information gathering
class InfoGatheringState(TypedDict):
    # The output from planning: list of information needs
    information_needs: List[Dict[str, str]]  # each has {summary, description, tag, stored}
    # We'll store results in a dictionary keyed by 'stored' field
    gathered_results: Dict[str, str]

def gather_info_for_all_needs(state: InfoGatheringState, config: RunnableConfig) -> InfoGatheringState:
    """
    Orchestrates the data gathering for each of the informationNeeds.
    This function can call a sub-graph (like vectorDB retrieval) for each requirement.
    For demonstration, we'll just pass the 'information_needs' to a subgraph call (analysis_module_vectordb).
    """
    from analysis_module_vectordb import build_vectordb_retrieval_graph

    results_storage = {}
    # Initialize the vectorDB retrieval graph
    retrieval_graph = build_vectordb_retrieval_graph()

    for info_need in state["information_needs"]:
        # Build sub-state for the retrieval
        sub_state = {
            "info_need_summary": info_need["summary"],
            "info_need_description": info_need["description"],
            "info_need_tag": info_need["tag"],
            "info_need_stored_key": info_need["stored"],
            "retrieved_text": ""
        }

        # Invoke the vector db retrieval sub-graph
        final_sub_state = retrieval_graph.invoke(sub_state)
        # Store the retrieved_text in results_storage
        results_storage[info_need["stored"]] = final_sub_state.get("retrieved_text", "")
    
    # Update the main state with all collected results
    state["gathered_results"] = results_storage
    return state

def build_info_gathering_orchestration_graph() -> StateGraph:
    """
    Builds a simple graph that collects info for each item using a sub-graph.
    """
    from analysis_module_vectordb import build_vectordb_retrieval_graph  # Just referencing

    builder = StateGraph(InfoGatheringState)
    builder.add_node("gather_info_for_all_needs", gather_info_for_all_needs)

    builder.add_edge(START, "gather_info_for_all_needs")
    builder.add_edge("gather_info_for_all_needs", END)

    return builder.compile()

def run_information_gathering_workflow(information_needs: List[Dict[str, str]]) -> dict:
    """
    High-level function that sets up the graph state with the information needs,
    then runs the gather_info_for_all_needs node. Returns all gathered results.
    """
    graph = build_info_gathering_orchestration_graph()
    initial_state: InfoGatheringState = {
        "information_needs": information_needs,
        "gathered_results": {}
    }
    final_state = graph.invoke(initial_state)
    return final_state

if __name__ == "__main__":
    # Example usage
    example_information_needs = [
        {
            "summary": "Total revenue for last fiscal year",
            "description": "Exact figure, with reference to official statement",
            "tag": "factual",
            "stored": "last_fiscal_revenue"
        },
        {
            "summary": "Discussion of major risk factors",
            "description": "A broad overview of the main risk factor items",
            "tag": "broad",
            "stored": "risk_factors_overview"
        }
    ]

    results = run_information_gathering_workflow(example_information_needs)
    print("\nGathered Results:")
    for key, value in results["gathered_results"].items():
        print(f"{key}: {value}")

---

<filepath>agent_retrieval.py</filepath>

import os
import sys
import logging
from typing import List, Dict, Any, TypedDict, Optional
from datetime import datetime
from dotenv import load_dotenv

try:
    import google.generativeai as genai
except ImportError:
    raise ImportError("Please install google-generativeai via: pip install google-generativeai")

try:
    from pinecone import Pinecone
except ImportError:
    raise ImportError("Please install Pinecone via: pip install pinecone")

from langgraph.graph.state import StateGraph, START, END
from langchain_core.runnables import RunnableConfig

# Load environment variables from .env file
load_dotenv()

# Standard 10-K item headings and approximate info stored
TENK_ITEM_HEADINGS = {
    "Item 1. Business": "Description of business operations, main products and services, revenue streams, competitive landscape, market share, industry trends, key customers, suppliers, seasonality, government regulations, intellectual property, research and development, employees, geographic markets, corporate structure, subsidiaries, company history, business strategy, recent acquisitions or divestitures",
    "Item 1A. Risk Factors": "Details about major risks and uncertainties for the business, economic risks, market risks, operational risks, financial risks, regulatory risks, cybersecurity risks, environmental risks, competition risks, litigation risks, reputational risks, supply chain risks, technological risks, geopolitical risks, pandemic-related risks",
    "Item 1B. Unresolved Staff Comments": "Any unresolved comments or issues raised by the SEC staff on the company's previous filings",
    "Item 2. Properties": "Information about property, plants, and equipment, real estate holdings, leased properties, manufacturing facilities, distribution centers, office locations, land ownership, property values, capacity utilization",
    "Item 3. Legal Proceedings": "Any ongoing legal matters or litigation, pending lawsuits, regulatory proceedings, environmental litigation, patent disputes, class action suits, settlement agreements",
    "Item 4. Mine Safety Disclosures": "Mine safety information if applicable, violations, citations, legal actions related to mine safety",
    "Item 5. Market for Registrant's Common Equity": "Stock market details, holder information, dividends, stock price history, stock repurchase programs, equity compensation plans, securities authorized for issuance, performance graph comparing stock performance to market indices",
    "Item 6. None": "This item is currently reserved by the SEC for future use",
    "Item 7. Management's Discussion and Analysis of Financial Condition and Results of Operations": "Management analysis, business overview, future outlook, key performance indicators, liquidity and capital resources, results of operations, critical accounting estimates, off-balance sheet arrangements, contractual obligations, market risk disclosures, segment reporting, trends affecting the business",
    "Item 7A. Quantitative and Qualitative Disclosures About Market Risk": "Analysis of market risks, including interest rate risk, foreign currency risk, commodity price risk, equity price risk, credit risk, hedging activities",
    "Item 8. Financial Statements and Supplementary Data": "Audited financial statements, including Balance Sheet, Income Statement, Statement of Cash Flows, Statement of Stockholders' Equity, footnotes, accounting policies, segment information, quarterly financial data, five-year financial summary",
    "Item 9. Changes in and Disagreements with Accountants": "Accounting or auditing-related changes, disagreements with previous auditors, reasons for auditor change",
    "Item 9A. Controls and Procedures": "Internal control, disclosure control procedures, management's assessment of internal controls, attestation report of the registered public accounting firm",
    "Item 9B. Other Information": "Miscellaneous or additional info not otherwise captured, subsequent events, material information not reported on Form 8-K",
    "Item 9C. Disclosure Regarding Foreign Jurisdictions that Prevent Inspections": "Information about operating in jurisdictions that prevent PCAOB inspections",
    "Item 10. Directors, Executive Officers and Corporate Governance": "Details about executives, board of directors, governance policies, code of ethics, board committees, corporate governance guidelines, director independence, executive biographies",
    "Item 11. Executive Compensation": "Information on compensation of officers and directors, salary, bonuses, stock awards, option grants, pension plans, employment agreements, compensation discussion and analysis (CD&A)",
    "Item 12. Security Ownership of Certain Beneficial Owners": "Information on major shareholders, ownership structure, beneficial ownership table, equity compensation plan information",
    "Item 13. Certain Relationships and Related Transactions": "Details of related party transactions or conflicts of interest, transactions with management or major shareholders, policies for approval of related party transactions",
    "Item 14. Principal Accountant Fees and Services": "Accounting fees, services performed by principal accountant, audit fees, audit-related fees, tax fees, all other fees, pre-approval policies for auditor services",
    "Item 15. Exhibits, Financial Statement Schedules": "Exhibits, schedules attached to the 10-K, list of subsidiaries, list of guarantor subsidiaries, consent of independent registered public accounting firm, power of attorney, certifications required under Sarbanes-Oxley Act",
    "Item 16. Form 10-K Summary": "Optional summary of the report if provided, key highlights from each section"
}

# -----------------------------
# LOGGING SETUP
# -----------------------------
def setup_logging():
    """Configure logging to both file and console with timestamps."""
    if not os.path.exists('logs'):
        os.makedirs('logs')

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = f'logs/search_query_{timestamp}.log'

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    logging.info(f"Query log file: {log_file}")

# -----------------------------
# STATE DEFINITION
# -----------------------------

class AgentState(TypedDict):
    user_query: str
    company_namespace: str              # Added company namespace
    relevant_item_headings: List[str]   
    available_subheadings: Dict[str, List[str]]
    search_configuration: Optional[Dict[str, Any]]
    pinecone_search_queries: List[str]
    matched_chunks: List[str]
    final_answer: Optional[str]

# -----------------------------
# NODE DEFINITIONS
# -----------------------------

def gemini_determine_headings(state: AgentState, config: RunnableConfig) -> AgentState:
    """
    Node #1: Use Gemini to interpret the user query and map it to one or more of the standard 10-K headings.
    """
    gemini_api_key = os.getenv("GEMINI_API_KEY")
    if not gemini_api_key:
        logging.error("Missing GEMINI_API_KEY environment variable.")
        return state

    # Configure gemini
    genai.configure(api_key=gemini_api_key)
    model = genai.GenerativeModel("gemini-1.5-flash")

    # Build a prompt that includes the standard headings and the user query
    headings_text = "\n".join([f"{k}: {v}" for k, v in TENK_ITEM_HEADINGS.items()])
    prompt = (
        "You are a helpful AI assistant. The user is asking about 10-K information. \n"
        "Below are the standard 10-K item headings and rough descriptions:\n"
        f"{headings_text}\n\n"
        "User query:\n"
        f"{state['user_query']}\n\n"
        "Instructions:\n"
        "1. Analyze the user query and identify the TOP 3 MOST RELEVANT item headings.\n"
        "2. Return ONLY a valid JSON object with exactly this structure:\n"
        "{\n"
        '    "relevant_headings": [\n'
        '        "Item X. Exact Heading Name",\n'
        '        "Item Y. Exact Heading Name",\n'
        '        "Item Z. Exact Heading Name"\n'
        "    ]\n"
        "}\n\n"
        "Requirements:\n"
        "- Include EXACTLY 3 headings\n"
        "- Each heading must be an EXACT MATCH from the list above\n"
        "- Return ONLY the JSON object, no other text\n"
        "- Ensure the JSON is properly formatted with double quotes"
    )

    try:
        response = model.generate_content(
            contents=prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=0.2,
                max_output_tokens=700
            )
        )
        
        # Parse the JSON response from the text
        import json
        try:
            response_text = response.text.strip()
            # If response is wrapped in ```json and ```, remove them
            if response_text.startswith('```json'):
                response_text = response_text[7:]
            if response_text.startswith('```'):
                response_text = response_text[3:]
            if response_text.endswith('```'):
                response_text = response_text[:-3]
            
            structured_response = json.loads(response_text.strip())
            relevant_headings = structured_response.get("relevant_headings", [])
            
            # Ensure headings are recognized
            recognized_headings = [
                h.strip() for h in relevant_headings 
                if h.strip() in TENK_ITEM_HEADINGS
            ][:3]  # Limit to 3 even if somehow more are returned

            logging.info(f"\nQUERY: {state['user_query']}")
            logging.info("\nRELEVANT ITEM HEADINGS:")
            for h in recognized_headings:
                logging.info(f"- {h}")
            
            state["relevant_item_headings"] = recognized_headings
            return state
            
        except json.JSONDecodeError as je:
            logging.error(f"Failed to parse JSON response: {je}")
            return state
            
    except Exception as e:
        logging.error(f"Error during heading determination: {e}")
        return state


def retrieve_metadata_with_zero_vector(state: AgentState, config: RunnableConfig) -> AgentState:
    """
    Node #2: Use a zero vector query to retrieve available subheadings for each relevant heading.
    """
    pinecone_api_key = os.getenv("PINECONE_API_KEY")
    if not pinecone_api_key:
        logging.error("Missing PINECONE_API_KEY environment variable.")
        return state

    pc = Pinecone(api_key=pinecone_api_key)
    index = pc.Index(host="https://financialdocs-ij61u7y.svc.aped-4627-b74a.pinecone.io")
    namespace = state["company_namespace"]

    # Create a zero vector with same dimensions as our embeddings (1024 for multilingual-e5-large)
    zero_vector = [0.0] * 1024
    available_subheadings = {}

    logging.info("\nSUBHEADINGS BY ITEM:")
    # Query once per relevant heading
    for heading in state["relevant_item_headings"]:
        try:
            metadata_filter = {
                "top_level_heading": heading
            }
            
            results = index.query(
                namespace=namespace,
                vector=zero_vector,
                top_k=100,
                include_values=False,
                include_metadata=True,
                filter=metadata_filter
            )
            
            logging.info(f"\n{heading}:")
            if not results or "matches" not in results:
                logging.info("└── No subheadings found")
                available_subheadings[heading] = []
                continue

            matches = results.get("matches", [])
            subheadings = set()
            for match in matches:
                meta = match.get("metadata", {})
                subheading = meta.get("subheading", "").strip()
                if subheading:
                    subheadings.add(subheading)
            
            available_subheadings[heading] = sorted(list(subheadings))
            
            if subheadings:
                for i, sh in enumerate(sorted(subheadings), 1):
                    prefix = "└──" if i == len(subheadings) else "├──"
                    logging.info(f"{prefix} {sh}")
            else:
                logging.info("└── No subheadings found")

        except Exception as e:
            logging.error(f"Error retrieving metadata for {heading}: {e}")
            available_subheadings[heading] = []
            continue

    state["available_subheadings"] = available_subheadings
    return state

def configure_search_with_gemini(state: AgentState, config: RunnableConfig) -> AgentState:
    """
    Node #3: Use Gemini to analyze the query and available metadata to configure the search.
    The search strategy combines:
    1. First two item headings without subheading filters (for broader context)
    2. Up to two very specific heading + subheading combinations (for targeted results)
    """
    gemini_api_key = os.getenv("GEMINI_API_KEY")
    if not gemini_api_key:
        logging.error("Missing GEMINI_API_KEY environment variable.")
        return state

    genai.configure(api_key=gemini_api_key)
    model = genai.GenerativeModel("gemini-1.5-flash")

    # Get first two headings for broad context searches
    broad_headings = state["relevant_item_headings"][:2]
    
    # Build context about available metadata
    metadata_context = "Available headings and subheadings:\n"
    for heading, subheadings in state["available_subheadings"].items():
        metadata_context += f"\n{heading}:\n"
        for subheading in subheadings:
            metadata_context += f"  - {subheading}\n"

    prompt = (
        "You are a search configuration expert. Given a user query and available metadata fields, "
        "determine up to TWO very specific heading + subheading combinations that are most relevant.\n\n"
        f"User Query: {state['user_query']}\n\n"
        f"{metadata_context}\n"
        "Instructions:\n"
        "1. Analyze the query and available metadata.\n"
        "2. Identify up to TWO very specific heading + subheading combinations that are HIGHLY relevant.\n"
        "3. Return a JSON object with this structure:\n"
        "{\n"
        '    "specific_searches": [\n'
        "        {\n"
        '            "heading": "Item X...",\n'
        '            "subheading": "Exact Subheading Name"\n'
        "        },\n"
        "        ...\n"
        "    ]\n"
        "}\n\n"
        "IMPORTANT:\n"
        "- Return 0-2 specific searches (only if highly confident)\n"
        "- Each subheading must EXACTLY match one from the available ones\n"
        "- Only include combinations that are HIGHLY likely to contain relevant information\n"
        "- Quality over quantity - better to return fewer, more precise matches\n"
    )

    try:
        response = model.generate_content(
            contents=prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=0.2,
                max_output_tokens=1000
            )
        )
        
        # Parse the JSON response
        import json
        try:
            response_text = response.text.strip()
            if response_text.startswith('```json'):
                response_text = response_text[7:]
            if response_text.startswith('```'):
                response_text = response_text[3:]
            if response_text.endswith('```'):
                response_text = response_text[:-3]
            
            specific_searches = json.loads(response_text.strip())
            
            # Build final search configuration
            search_config = {"heading_configs": []}
            
            # Add broad context searches (first two headings without subheading filters)
            for heading in broad_headings:
                search_config["heading_configs"].append({
                    "heading": heading,
                    "use_subheading_filter": False
                })
            
            # Add specific searches with subheading filters
            for specific in specific_searches.get("specific_searches", []):
                heading = specific.get("heading")
                subheading = specific.get("subheading")
                
                # Validate the combination exists in our available metadata
                if (heading in state["available_subheadings"] and 
                    subheading in state["available_subheadings"][heading]):
                    search_config["heading_configs"].append({
                        "heading": heading,
                        "use_subheading_filter": True,
                        "relevant_subheadings": [subheading]
                    })
            
            logging.info("\nSEARCH CONFIGURATION:")
            # Log broad context searches
            logging.info("\nBroad context searches:")
            for config in search_config["heading_configs"][:2]:
                logging.info(f"- {config['heading']} (no subheading filter)")
            
            # Log specific searches
            specific_searches = search_config["heading_configs"][2:]
            if specific_searches:
                logging.info("\nSpecific searches:")
                for config in specific_searches:
                    if config.get("use_subheading_filter") and config.get("relevant_subheadings"):
                        logging.info(f"- {config['heading']}")
                        logging.info(f"  Subheading: {config['relevant_subheadings'][0]}")
            
            state["search_configuration"] = search_config
            return state
            
        except json.JSONDecodeError as je:
            logging.error("Failed to parse search configuration")
            state["search_configuration"] = {"heading_configs": [
                {"heading": h, "use_subheading_filter": False}
                for h in broad_headings
            ]}
            return state
            
    except Exception as e:
        logging.error("Error during search configuration")
        state["search_configuration"] = {"heading_configs": [
            {"heading": h, "use_subheading_filter": False}
            for h in broad_headings
        ]}
        return state

def pinecone_vector_search(state: AgentState, config: RunnableConfig) -> AgentState:
    """
    Node #4: Perform semantic search in Pinecone with metadata filtering based on LLM configuration.
    """
    pinecone_api_key = os.getenv("PINECONE_API_KEY")
    if not pinecone_api_key:
        logging.error("Missing PINECONE_API_KEY environment variable.")
        return state

    pc = Pinecone(api_key=pinecone_api_key)
    index = pc.Index(host="https://financialdocs-ij61u7y.svc.aped-4627-b74a.pinecone.io")
    namespace = state["company_namespace"]

    all_chunks = []
    user_query = state["user_query"]
    search_config = state.get("search_configuration", {"heading_configs": []})

    # Create one embedding for the user query
    try:
        query_embedding_list = pc.inference.embed(
            model="multilingual-e5-large",
            inputs=[user_query],
            parameters={"input_type": "query", "truncate": "END"}
        )
    except Exception as e:
        logging.error(f"Error embedding query {user_query}: {e}")
        return state

    if not query_embedding_list or not query_embedding_list.data:
        logging.warning(f"No query embedding returned from Pinecone inference for: {user_query}")
        return state

    query_vector = query_embedding_list[0].values
    logging.info(f"Embedding generated for query: {user_query[:80]}...")

    # Search based on configuration
    for heading_config in search_config.get("heading_configs", []):
        try:
            heading = heading_config.get("heading")
            use_subheading_filter = heading_config.get("use_subheading_filter", False)
            relevant_subheadings = heading_config.get("relevant_subheadings", [])

            # Build metadata filter
            metadata_filter = {
                "top_level_heading": heading
            }

            # Add subheading filter if configured
            if use_subheading_filter and relevant_subheadings:
                metadata_filter["subheading"] = {"$in": relevant_subheadings}
                logging.info(f"Searching {heading} with subheading filter: {relevant_subheadings}")
            else:
                logging.info(f"Searching {heading} without subheading filter")
            
            results = index.query(
                namespace=namespace,
                vector=query_vector,
                top_k=5,
                include_values=False,
                include_metadata=True,
                filter=metadata_filter
            )
            
            if not results or "matches" not in results:
                logging.warning(f"No results for heading: {heading}")
                continue

            matches = results["matches"] or []
            if matches:
                logging.info(f"Found {len(matches)} matches for {heading}")
            
            for match in matches:
                meta = match.get("metadata", {})
                chunk_info = {
                    "text": meta.get("chunk_text", ""),
                    "heading": meta.get("top_level_heading", ""),
                    "subheading": meta.get("subheading", ""),
                    "score": match.get("score", 0)
                }
                if chunk_info["text"]:
                    all_chunks.append(chunk_info)

        except Exception as e:
            logging.error(f"Error during Pinecone query for heading {heading}: {e}")
            continue

    # Remove all logging except final count
    logging.info(f"\nFound {len(all_chunks)} relevant chunks")
    state["matched_chunks"] = all_chunks
    return state


def interpret_results_with_gemini(state: AgentState, config: RunnableConfig) -> AgentState:
    """
    Node #4: Now that we have matched chunks, we call Gemini to generate a final answer.
    """
    gemini_api_key = os.getenv("GEMINI_API_KEY")
    if not gemini_api_key:
        logging.error("Missing GEMINI_API_KEY environment variable.")
        return state

    genai.configure(api_key=gemini_api_key)
    model = genai.GenerativeModel("gemini-1.5-flash")

    query_text = state["user_query"]
    matched_chunks = state["matched_chunks"]
    context_text = ""
    for i, chunk in enumerate(matched_chunks, start=1):
        context_text += (
            f"\n[CHUNK {i}]:\n"
            f"Heading: {chunk['heading']}\n"
            f"Subheading: {chunk['subheading']}\n"
            f"Content: {chunk['text']}\n"
            f"Relevance Score: {chunk['score']:.2f}\n"
        )

    prompt = (
        "You are a professional financial analyst providing detailed analysis based on 10-K filings. "
        "For the given user query, I will provide relevant sections from the company's 10-K report. "
        "\nWhen answering:\n"
        "1. Structure your response in a professional, analytical tone.\n"
        "2. Focus on concrete facts, statistics, and specific details from the source material.\n"
        "3. For each piece of information, add a reference number in square brackets [1] within the text.\n"
        "4. Present your analysis in a clear, logical flow.\n"
        "5. After your main analysis, include a 'Sources:' section that lists all references.\n\n"
        "Format your response like this:\n\n"
        "ANALYSIS:\n"
        "[Main analysis text with [1], [2], etc. as references]\n\n"
        "SOURCES:\n"
        "[1] [heading], [subheading], [...relevant quote...] \n"
        "[2] [heading], [subheading], [...relevant quote...]\n"
        "...\n\n"
        "User Query: "
        f"{query_text}\n\n"
        f"Context:\n{context_text}\n\n"
        "Remember to maintain a professional tone and ensure every significant piece of information is properly referenced."
    )

    try:
        response = model.generate_content(
            contents=prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=0.2,
                max_output_tokens=1700
            )
        )
        final_text = response.text
        logging.info("\nFINAL ANSWER:")
        logging.info("=" * 80)
        logging.info(final_text)
        logging.info("=" * 80)
        state["final_answer"] = final_text
        return state
    except Exception as e:
        logging.error(f"Error generating final answer: {e}")
        state["final_answer"] = "I'm sorry, I encountered an error while generating the answer."
        return state

# -----------------------------
# GRAPH BUILDING
# -----------------------------
def build_retrieval_graph() -> StateGraph:
    """
    Build the LangGraph for multi-step retrieval.
    """
    builder = StateGraph(AgentState)

    # Add nodes
    builder.add_node("gemini_determine_headings", gemini_determine_headings)
    builder.add_node("retrieve_metadata_with_zero_vector", retrieve_metadata_with_zero_vector)
    builder.add_node("configure_search_with_gemini", configure_search_with_gemini)
    builder.add_node("pinecone_vector_search", pinecone_vector_search)
    builder.add_node("interpret_results_with_gemini", interpret_results_with_gemini)

    # Edges
    builder.add_edge(START, "gemini_determine_headings")
    builder.add_edge("gemini_determine_headings", "retrieve_metadata_with_zero_vector")
    builder.add_edge("retrieve_metadata_with_zero_vector", "configure_search_with_gemini")
    builder.add_edge("configure_search_with_gemini", "pinecone_vector_search")
    builder.add_edge("pinecone_vector_search", "interpret_results_with_gemini")
    builder.add_edge("interpret_results_with_gemini", END)

    graph = builder.compile()
    return graph

# -----------------------------
# MAIN
# -----------------------------
def main():
    setup_logging()

    if len(sys.argv) < 3:
        logging.error("Usage: python agent_retrieval.py \"<company_name>\" \"<query_text>\"")
        return

    company_name = sys.argv[1].strip().lower()
    user_query = sys.argv[2].strip()

    # Generate clean namespace (lowercase alphanumeric only)
    import re
    namespace = re.sub(r'[^a-z0-9]', '', company_name)[:63]  # Max 63 chars for Pinecone

    # Setup initial state
    initial_state: AgentState = {
        "user_query": user_query,
        "company_namespace": namespace,
        "relevant_item_headings": [],
        "available_subheadings": [],
        "search_configuration": None,
        "pinecone_search_queries": [],
        "matched_chunks": [],
        "final_answer": None
    }

    # Build graph and run
    graph = build_retrieval_graph()
    final_state = graph.invoke(initial_state)

    # Get the final answer
    final_answer = final_state.get("final_answer", "No final answer.")
    
    # Print a separator for better readability
    print("\n" + "="*80 + "\n" + "FINAL RESULT:" + "\n" + "="*80)
    print(final_answer)
    
    # Log the result without printing it again
    logging.info("Final result generated successfully.")

if __name__ == '__main__':
    main()

---
