<filepath>prompt_builder.md</filepath>

<user_instructions>

Help me write this input into a structured and professional software architecture documentation that can be provided to both developers and AI coding assistants for context. 

Additionally, provide a detailled and step-by-step guide for implementing the whole system in code and for production (only the functionality, not the UX or anything else).

## Core Modules

1. Import Pipeline (details in @import_pipeline.py and @import_0_search.py, @import_1_retrieval.py, @import_2_parsing.py, @import_3_indexing.py)
2. Analysis Pipeline (initial draftdetails in @analysis_module_plan.py and @analysis_module_infogathering_orchestrator.py, sub-graph vector search in @analysis_module_vectordb.py and analysis_module_web_search.py)
3. Execution Pipeline (intiial draft details in @analysis_module_execution.py)

# Functionality
Import Pipeline findes, downloads, chunks and indexes the 10k financial documents.

Analysis Pipeline:
1. First, user query is analyzed and a plan is created
2. Orchestrator parses the plan and passes it (in parallel) into the information gathering sub-graphs
3. Each Subgraph (vector search, web search, etc) will receive information needs, choose a search plan and transform the information needs into queries, then execute the search and return the results to the orchestrator
4. Orchestrator will evaluate if the information gathered is enough to answer the user query, if not, it will refine the plan and pass it again into the information gathering sub-graphs
5. Once the information is enough, the orchestrator will pass the plan into the execution sub-graph
6. Execution sub-graph will parse the plan and the information gathered and if necessary re-fine the plan.
7. Execution will go step-by-step, each step receiving instructions, information gathered and the results and reasoning of the previous step (if any).
9. The executor will format the results and return the final answer to the user

</user_instructions>


---

<filepath>import_0_search.py</filepath>

import os
from datetime import datetime
import logging
from typing import Dict, Optional

try:
    from exa_py import Exa
except ImportError:
    raise ImportError("Please install exa-py via: pip install exa-py")

def setup_logging():
    """Configure basic logging to console"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(message)s'
    )

def search_10k(company_name: str, exa_api_key: str) -> Optional[Dict[str, str]]:
    """
    Search for a company's most recent 10-K using Exa's search API.
    
    Args:
        company_name: Name of the company to search for
        exa_api_key: Exa API key for authentication
    
    Returns:
        Dictionary containing metadata about the found document:
            - company_name: Name of the company
            - fiscal_year: Fiscal year of the document
            - document_url: URL of the source document
            - document_type: Type of document (10-K)
        Returns None if no document is found or on error
    """
    try:
        # Initialize Exa client
        exa = Exa(api_key=exa_api_key)
        doc_type = "Form 10-K annual report"
        full_query = f"{company_name} {doc_type}"

        # Search for the document
        search_result = exa.search_and_contents(
            full_query,
            type="auto",
            num_results=1,
            text=False,  # We don't need the text content
            category="financial report",
            start_published_date="2023-11-30T23:00:01.000Z"  # Only recent documents
        )

        if not search_result or not search_result.results or len(search_result.results) == 0:
            logging.error(f"No 10-K found for {company_name}")
            return None

        doc = search_result.results[0]
        
        # Extract fiscal year from published date
        published_date_str = doc.published_date if doc.published_date else ""
        try:
            fiscal_year = published_date_str[:4]
            if not fiscal_year.isdigit():
                fiscal_year = "Unknown"
        except:
            fiscal_year = "Unknown"
            
        document_url = doc.url if doc.url else ""
        if not document_url:
            logging.error("No URL found in search result")
            return None

        # Construct metadata
        metadata = {
            "company_name": company_name,
            "fiscal_year": fiscal_year,
            "document_url": document_url,
            "document_type": "10-K"
        }

        # Log the search results
        logging.info("\n=== SEARCH RESULTS ===")
        for key, value in metadata.items():
            logging.info(f"{key}: {value}")
        logging.info("====================")

        return metadata

    except Exception as e:
        logging.error(f"Error during search: {str(e)}")
        return None

def main():
    """Test the search functionality with a sample company."""
    setup_logging()
    
    # Get company name from command line or prompt
    if len(sys.argv) > 1:
        company_name = sys.argv[1]
    else:
        company_name = input("Enter company name (e.g., NVIDIA, Apple, Microsoft): ").strip()

    if not company_name:
        print("Company name cannot be empty")
        return

    # Get API key from environment or prompt
    exa_api_key = os.getenv("EXA_API_KEY")
    if not exa_api_key:
        exa_api_key = input("Enter your Exa API key: ").strip()
        if not exa_api_key:
            print("No API key provided")
            return

    # Perform the search
    metadata = search_10k(company_name, exa_api_key)
    if not metadata:
        print("\nSearch failed. No document found.")

if __name__ == "__main__":
    import sys
    main() 

---

<filepath>import_1_parse.py</filepath>

import os
from datetime import datetime
import requests
from dotenv import load_dotenv
from llama_parse import LlamaParse

def download_sec_filing_pdf(url: str, sec_api_key: str) -> str:
    """Download a SEC filing as PDF using the SEC API."""
    
    # Construct the SEC API URL
    api_url = "https://api.sec-api.io/filing-reader"
    params = {
        "token": sec_api_key,
        "url": url
    }
    
    print(f"Downloading PDF from SEC API for URL: {url}")
    response = requests.get(api_url, params=params)
    
    if response.status_code != 200:
        raise Exception(f"Failed to download PDF: {response.status_code} - {response.text}")
    
    # Save the PDF
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = "sec_pdfs"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    company_name = url.split('/')[-1].split('-')[0].upper()
    pdf_path = os.path.join(output_dir, f"{company_name}_10k_{timestamp}.pdf")
    
    with open(pdf_path, "wb") as f:
        f.write(response.content)
    
    print(f"PDF saved to: {pdf_path}")
    return pdf_path

def parse_pdf_to_markdown(pdf_path: str, llama_api_key: str) -> str:
    """Parse PDF to markdown using LlamaParse."""
    
    print("\n--- Starting LlamaParse parsing ---")
    
    # Initialize parser with markdown-optimized configuration
    parser = LlamaParse(
        api_key=llama_api_key,
        result_type="markdown",  # Use markdown format
        verbose=True,
        language="en",
        fast_mode=False,  # Disable fast mode for better accuracy
        split_by_page=False,  # Don't split content by page
        num_workers=1,  # Sequential processing for reliability
        do_not_unroll_columns=False,  # Better for financial tables
        skip_diagonal_text=True,  # Skip diagonal text which might be artifacts
        parsing_instructions="""
        1. Preserve all headings and subheadings in markdown format
        2. Use # for top-level headings (e.g., PART I, ITEM 1)
        3. Use ## for second-level headings
        4. Use ### for third-level headings and below
        5. Maintain the original heading text and formatting
        6. Ensure all headings start on a new line
        7. Preserve table formatting in markdown
        """
    )
    
    try:
        # Parse the PDF file
        documents = parser.load_data(pdf_path)
        
        if not documents or len(documents) == 0:
            raise Exception("No parsed content returned from LlamaParse")
            
        # Combine all document parts if multiple were returned
        parsed_output = ""
        for doc in documents:
            doc_text = doc.text if hasattr(doc, 'text') else str(doc)
            parsed_output += doc_text + "\n\n"
            
        # Save the markdown output
        output_dir = "parsed_results"
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        company_name = os.path.basename(pdf_path).split('_')[0]
        output_filename = os.path.join(output_dir, f"{company_name}_parsed_{timestamp}.md")
        
        with open(output_filename, "w", encoding="utf-8") as f:
            f.write(parsed_output)
            
        print(f"Parsed markdown saved to: {output_filename}")
        return output_filename
            
    except Exception as e:
        print(f"Error parsing with LlamaParse: {e}")
        return ""

def test_sec_download():
    """Test downloading a 10-K filing as PDF and parsing to markdown."""
    
    # Load environment variables
    load_dotenv()
    
    # Get API keys
    sec_api_key = os.getenv("SEC_API_KEY")
    llama_api_key = os.getenv("LLAMA_CLOUD_API_KEY")
    
    if not sec_api_key:
        raise ValueError("SEC_API_KEY not found in environment variables")
    if not llama_api_key:
        raise ValueError("LLAMA_CLOUD_API_KEY not found in environment variables")
    
    # Test URL - using Palantir's 10-K URL
    test_url = "https://www.sec.gov/Archives/edgar/data/1321655/000132165524000022/pltr-20231231.htm"
    
    try:
        # Step 1: Download the PDF
        pdf_path = download_sec_filing_pdf(test_url, sec_api_key)
        print(f"\nSuccessfully downloaded PDF to: {pdf_path}")
        
        # Step 2: Parse PDF to markdown
        markdown_path = parse_pdf_to_markdown(pdf_path, llama_api_key)
        if markdown_path:
            print(f"\nSuccessfully parsed PDF to markdown: {markdown_path}")
        else:
            print("\nFailed to parse PDF to markdown")
        
    except Exception as e:
        print(f"An error occurred: {e}")
        if hasattr(e, 'response'):
            print(f"Response details: {e.response.text if hasattr(e.response, 'text') else 'No response text'}")

if __name__ == "__main__":
    test_sec_download() 

---

<filepath>import_2_chunking.py</filepath>

import re
import json
from typing import List, Dict, Optional, Tuple, Any
from datetime import datetime
import os

# If we want to limit chunk size (approximate character-based threshold)
DEFAULT_MAX_CHUNK_SIZE = 300

# Regex pattern for matching Table of Contents heading (case insensitive)
RE_TABLE_OF_CONTENTS = re.compile(r"(?:TABLE\s+OF\s+CONTENTS|INDEX)", re.IGNORECASE)

# Regex for identifying lines that might be markdown headings: #, ##, ###, ...
RE_SUBHEADING = re.compile(r"^#+\s+(.*)$")

# Regex for identifying part headers (PART I, PART II, etc.)
RE_PART_HEADER = re.compile(r"^(?:#\s*)?PART\s+[IVX]+", re.IGNORECASE)

# Regex pattern for top-level Item headings: e.g. "Item 1.", "Item 1A."
RE_ITEM_HEADING = re.compile(r"(?i)^(?:#\s*)?(?:Item\s*(\d+[A-Za-z]?)\.?)\s*(.*)$")

# Standard 10-K Item headings
STANDARD_10K_HEADINGS = [
    "Item 1. Business",
    "Item 1A. Risk Factors",
    "Item 1B. Unresolved Staff Comments",
    "Item 1C. Cybersecurity",
    "Item 2. Properties",
    "Item 3. Legal Proceedings",
    "Item 4. Mine Safety Disclosures",
    "Item 5. Market for Registrant's Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities",
    "Item 6. [Reserved]",
    "Item 7. Management's Discussion and Analysis of Financial Condition and Results of Operations",
    "Item 7A. Quantitative and Qualitative Disclosures About Market Risk",
    "Item 8. Financial Statements and Supplementary Data",
    "Item 9. Changes in and Disagreements with Accountants on Accounting and Financial Disclosure",
    "Item 9A. Controls and Procedures",
    "Item 9B. Other Information",
    "Item 9C. Disclosure Regarding Foreign Jurisdictions that Prevent Inspections",
    "Item 10. Directors, Executive Officers and Corporate Governance",
    "Item 11. Executive Compensation",
    "Item 12. Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters",
    "Item 13. Certain Relationships and Related Transactions, and Director Independence",
    "Item 14. Principal Accountant Fees and Services",
    "Item 15. Exhibits, Financial Statement Schedules",
    "Item 16. Form 10-K Summary"
]

# Regex pattern to detect tables. We try to capture blocks of lines starting with "|"
RE_TABLE_LINE = re.compile(r"^\|.*\|$")

def extract_document_metadata(text: str) -> Dict[str, str]:
    """
    Extract some metadata from the document header.
    We'll guess the company_name from a line starting with '#' that is not Firecrawl or Analysis, etc.
    We'll also check for a "Document Type:" line if present.
    """
    lines = text.split("\n")
    metadata = {
        "company_name": "Unknown",
        "document_type": "Unknown"
    }
    for i, line in enumerate(lines[:40]):
        tline = line.strip()
        if tline.startswith("# ") and "Firecrawl" not in tline and "Analysis" not in tline:
            # For now, take the first occurrence as the Company Name.
            metadata["company_name"] = tline.replace("#", "").strip()
        elif "Document Type:" in tline:
            # e.g. "Document Type: Form 10-K annual report"
            doc_type_part = tline.split("Document Type:", 1)[1].strip()
            metadata["document_type"] = doc_type_part
    return metadata

def is_likely_toc_entry(line: str) -> bool:
    """
    Check if a line looks like a TOC entry (item heading followed by a page number).
    """
    line = line.strip()
    # Check if line ends with a number after some whitespace
    if re.search(r'\s+\d+$', line):
        # Check if it starts with an item heading
        item_info = detect_item_heading(line)
        if item_info and item_info['full_heading'] in STANDARD_10K_HEADINGS:
            return True
    return False

def find_start_of_content(text: str) -> int:
    """
    Find where the actual content starts by:
    1. Looking for dense standard item headings (indicating TOC)
    2. Finding either Item 1. Business or Forward-Looking Statements
    Returns the index where content should start.
    """
    lines = text.split('\n')
    
    # Find first non-XBRL line
    start_idx = 0
    for i, line in enumerate(lines):
        line = line.strip()
        if line == "---":
            start_idx = i + 1
            break
        if not line or "http://" in line or "Member" in line:
            continue
        start_idx = i
        break
    
    # Look for dense item headings in the first portion (indicating TOC)
    item_heading_positions = []
    for i in range(start_idx, min(start_idx + 200, len(lines))):
        line = lines[i].strip()
        item_info = detect_item_heading(line)
        if item_info and item_info['full_heading'] in STANDARD_10K_HEADINGS:
            item_heading_positions.append(i)
            # If we find at least 5 item headings within 50 lines, it's likely the TOC
            if len(item_heading_positions) >= 5 and item_heading_positions[-1] - item_heading_positions[0] <= 50:
                # Found TOC, now look for start of content
                for j in range(i + 1, len(lines)):
                    line = lines[j].strip()
                    # Check for Forward-Looking Statements
                    if line.lower() == "# forward-looking statements":
                        return j
                    # Check for real Item 1. Business (not just TOC entry)
                    item_info = detect_item_heading(line)
                    if (item_info and 
                        item_info['full_heading'] == "Item 1. Business" and 
                        len(line.split()) > 3):  # More than just "Item 1. Business"
                        return j
    
    # If we didn't find TOC or content start, return original start
    return start_idx

def normalize_text(text: str) -> str:
    """
    Normalize text by:
    1. Converting to lowercase
    2. Replacing various apostrophe types with a standard one
    3. Removing extra whitespace
    """
    text = text.lower()
    # Replace various apostrophe types with a standard one
    text = re.sub(r"['`'']", "'", text)
    # Normalize whitespace
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def detect_item_heading(line: str) -> Optional[Dict[str, str]]:
    """
    If line matches 'Item X. [title]', parse it out.
    Returns dictionary with item_number, item_title, full_heading if matched.
    More flexible with formatting but still reliable.
    """
    # Remove any markdown heading prefix for matching
    clean_line = re.sub(r'^#+\s*', '', line.strip())
    
    # Basic pattern to match "Item X." at start (case insensitive)
    m = re.match(r'(?i)item\s+(\d+[A-Za-z]?)\.?\s*(.*)', clean_line)
    if not m:
        return None

    # group(1) is the item number, group(2) is the item title
    item_number = m.group(1).upper().strip()
    item_title = m.group(2).strip()
    
    # Find matching standard heading by normalizing both strings
    normalized_title = normalize_text(item_title)
    
    # First try exact match with standard headings
    standard_heading = None
    for heading in STANDARD_10K_HEADINGS:
        if f"Item {item_number}." in heading:
            standard_heading = heading
            break
    
    if standard_heading:
        # Use the standard heading's title
        item_title = standard_heading.split(".", 1)[1].strip()
    else:
        # If no exact match but we have a valid item number, use the standard heading
        for heading in STANDARD_10K_HEADINGS:
            if f"Item {item_number}." in heading:
                item_title = heading.split(".", 1)[1].strip()
                break
    
    full_heading = f"Item {item_number}. {item_title}".strip()
    
    # Additional validation: if this is meant to be a standard item heading
    # but doesn't match our standard list exactly, it might be a false positive
    if item_number.isdigit() or (len(item_number) > 1 and item_number[:-1].isdigit()):
        if not any(h.startswith(f"Item {item_number}.") for h in STANDARD_10K_HEADINGS):
            return None
    
    return {
        "item_number": item_number,
        "item_title": item_title,
        "full_heading": full_heading
    }

def detect_subheading(line: str) -> Optional[str]:
    """
    If line matches a sub-heading in markdown (#, ##, etc.), return the text.
    Otherwise return None.
    We'll exclude lines that are empty or should be ignored based on our ignore rules.
    """
    m = RE_SUBHEADING.match(line)
    if not m:
        return None
    heading_text = m.group(1).strip()
    
    # First check if this is a standard item heading or financial statement
    item_info = detect_item_heading(heading_text)
    if item_info:
        if item_info['item_number'] == "8":
            # For financial statements, use the heading text as the subheading
            return heading_text
        elif item_info['full_heading'] in STANDARD_10K_HEADINGS:
            return None  # Never treat standard item headings as subheadings
        
    # Special handling for financial statement sections
    financial_statement_patterns = [
        r'(?i)consolidated\s+statements?\s+of\s+(?:income|operations|comprehensive\s+income|financial\s+position|cash\s+flows|shareholders\'\s+equity)',
        r'(?i)consolidated\s+balance\s+sheets?',
        r'(?i)report\s+of\s+independent\s+registered\s+public\s+accounting\s+firm',
        r'(?i)notes\s+to\s+(?:the\s+)?consolidated\s+financial\s+statements',
        r'(?i)opinions\s+on\s+the\s+financial\s+statements',
        r'(?i)basis\s+for\s+opinions',
        r'(?i)critical\s+audit\s+matters'
    ]
    
    # Check if this is a financial statement heading
    for pattern in financial_statement_patterns:
        if re.search(pattern, heading_text, re.IGNORECASE):
            return heading_text
            
    # Use our comprehensive ignore rules
    if should_ignore_heading(heading_text):
        return None
        
    return heading_text

def is_table_line(line: str) -> bool:
    """
    Check if the line is a typical table format line:
    starts with '|' and ends with '|', or at least has '|---|'.
    We'll rely on the simpler pattern RE_TABLE_LINE here.
    """
    return bool(RE_TABLE_LINE.match(line.strip()))

def finalize_chunk_text_and_determine_type(text_lines: List[str]) -> Tuple[str, str]:
    """
    Given the chunk text lines, decide if it's 'table', 'text', or 'mix'.
    Return (joined_text, chunk_type).
    If all lines are table lines, type=table.
    If all lines are non-table lines, type=text.
    Else mix.
    """
    # Filter out empty lines for table detection logic
    non_empty = [ln for ln in text_lines if ln.strip()]
    if not non_empty:
        # it's basically empty, we'll treat it as text
        return ("\n".join(text_lines).strip(), "text")

    has_table = False
    has_text = False
    for ln in non_empty:
        if is_table_line(ln):
            has_table = True
        else:
            has_text = True
        if has_table and has_text:
            return ("\n".join(text_lines).strip(), "mix")
    # if we get here, it's all text or all table
    if has_table and not has_text:
        return ("\n".join(text_lines).strip(), "table")
    return ("\n".join(text_lines).strip(), "text")

def detect_standard_item_heading(line: str) -> Optional[str]:
    """
    Detect if a line is a standard 10-K Item heading.
    Returns the standardized heading if found, None otherwise.
    """
    print(f"Checking line: {repr(line)}")  # Debug
    
    # Only match lines that start with exactly "# Item "
    if not line.startswith("# Item "):
        print("  Not starting with '# Item '")  # Debug
        return None
        
    # Remove the "# " prefix
    line = line[2:].strip()
    print(f"  After prefix removal: {repr(line)}")  # Debug
    
    # Check if this exact heading is in our standard list
    if line in STANDARD_10K_HEADINGS:
        print(f"  Found match: {line}")  # Debug
        return line
            
    print("  No match in standard headings")  # Debug
    return None

def chunkify_by_items(text: str, toc_item_headings: List[str] = None) -> List[Dict[str, str]]:
    """
    Split text into chunks based on Item headings.
    Each chunk should start with a standard 10-K Item heading.
    """
    chunks = []
    lines = text.split("\n")
    current_chunk_lines = []
    current_item_heading = None
    found_forward_looking = False
    in_toc = True  # Start assuming we're in TOC
    
    # Initialize with first TOC item if available
    first_toc_item = toc_item_headings[0] if toc_item_headings and len(toc_item_headings) > 0 else None
    
    for i, line in enumerate(lines):
        line_strip = line.strip()
        
        # Check for Forward-Looking Statements
        if line_strip.lower() == "# forward-looking statements":
            found_forward_looking = True
            # Save any previous content as TOC if we're still in TOC mode
            if in_toc and current_chunk_lines:
                chunks.append({
                    "heading": "Table of Contents",
                    "text": "\n".join(current_chunk_lines).strip()
                })
                current_chunk_lines = []
                in_toc = False
            current_item_heading = "Item 1. Business"
            current_chunk_lines.append(line)
            continue
        
        # Check if this is a new Item heading
        item_heading_info = detect_item_heading(line_strip)
        
        if item_heading_info and item_heading_info['full_heading'] in STANDARD_10K_HEADINGS:
            # If this is a real item heading (not just TOC entry)
            if len(line_strip.split()) > 3 and not is_likely_toc_entry(line_strip):
                # If we're still in TOC mode, save TOC first
                if in_toc and current_chunk_lines:
                    chunks.append({
                        "heading": "Table of Contents",
                        "text": "\n".join(current_chunk_lines).strip()
                    })
                    current_chunk_lines = []
                    in_toc = False
                # If we have a previous non-TOC chunk, save it
                elif not in_toc and current_chunk_lines:
                    chunks.append({
                        "heading": current_item_heading or first_toc_item or "Table of Contents",
                        "text": "\n".join(current_chunk_lines).strip()
                    })
                    current_chunk_lines = []
                
                # Start new chunk
                current_chunk_lines = [line]
                current_item_heading = item_heading_info["full_heading"]
                found_forward_looking = False  # Reset when we hit a new item
            else:
                # This is a TOC entry, just add it to current chunk
                current_chunk_lines.append(line)
        else:
            # If we're after Forward-Looking Statements but before next item heading
            if found_forward_looking and not current_item_heading:
                current_item_heading = "Item 1. Business"
            
            # Add to current chunk
            current_chunk_lines.append(line)
    
    # Don't forget the last chunk
    if current_chunk_lines:
        chunks.append({
            "heading": current_item_heading or first_toc_item or "Table of Contents",
            "text": "\n".join(current_chunk_lines).strip()
        })
    
    return chunks

def should_ignore_heading(heading: str) -> bool:
    """
    Determine if a heading should be ignored for chunking purposes.
    """
    if not heading or not heading.strip():
        return True
        
    # Store original heading for case-sensitive checks
    original_heading = heading.strip()
    
    # Remove any markdown heading markers and clean the text
    heading = re.sub(r'^#+\s*', '', original_heading.strip()).lower()
    
    # Never ignore certain important financial statement headings
    financial_statement_patterns = [
        r'(?i)consolidated\s+statements?\s+of\s+(?:income|operations|comprehensive\s+income|financial\s+position|cash\s+flows|shareholders\'\s+equity)',
        r'(?i)report\s+of\s+independent\s+registered\s+public\s+accounting\s+firm',
        r'(?i)notes\s+to\s+(?:the\s+)?consolidated\s+financial\s+statements',
        r'(?i)opinions\s+on\s+the\s+financial\s+statements',
        r'(?i)basis\s+for\s+opinions',
        r'(?i)critical\s+audit\s+matters'
    ]
    
    for pattern in financial_statement_patterns:
        if re.search(pattern, heading, re.IGNORECASE):
            return False  # Don't ignore these important headings
    
    # Ignore "None" as a heading
    if heading == "none":
        return True
    
    # Ignore any variation of "Table of Contents" or "Index"
    if RE_TABLE_OF_CONTENTS.search(heading):
        return True
    
    # Ignore PART headings in any format (e.g., "PART I", "Part II", "PART 1", etc.)
    if re.match(r'^part\s*(?:[IVXivx]+|\d+)(?:\s|$)', heading, re.IGNORECASE):
        return True
    
    # Ignore if it's a standard item heading (these should be handled by item heading logic)
    if heading.startswith('item ') and any(h.lower().startswith(heading) for h in STANDARD_10K_HEADINGS):
        return True
        
    # Ignore if it's just a repetition of a standard item heading
    if re.match(r'^item\s+\d+[A-Za-z]?\.?\s+', heading):
        item_match = re.match(r'^item\s+(\d+[A-Za-z]?)\.?', heading)
        if item_match:
            item_num = item_match.group(1)
            for std_heading in STANDARD_10K_HEADINGS:
                std_heading_lower = std_heading.lower()
                if (std_heading_lower.startswith(f'item {item_num.lower()}') and 
                    original_heading.isupper() and 
                    heading.replace(' ', '') in std_heading_lower.replace(' ', '')):
                    return True
    
    # Ignore financial metadata patterns
    financial_metadata_patterns = [
        r'^\(in\s+(?:millions|thousands|billions)\)',
        r'^\(in\s+(?:millions|thousands|billions),\s+except.*\)',
        r'^see\s+accompanying\s+notes',
        r'^see\s+notes?\s+to',
        r'^weighted\s+average',
        r'^year[s]?\s+ended',
        r'^as\s+of\s+(?:and\s+for)?',
        r'^for\s+the\s+(?:three|six|nine|twelve)\s+months?\s+ended',
        r'^for\s+the\s+years?\s+ended',
        r'^for\s+the\s+periods?\s+(?:ended|presented)',
        r'^consolidated\s+balance\s+sheets?\s+data',
        r'^selected\s+(?:consolidated\s+)?financial\s+data',
        r'^supplementary\s+financial\s+information'
    ]
    
    if any(re.match(pattern, heading, re.IGNORECASE) for pattern in financial_metadata_patterns):
        return True
    
    # Ignore date patterns
    date_patterns = [
        r'^(?:january|february|march|april|may|june|july|august|september|october|november|december)\s+\d{1,2},?\s+\d{4}$',
        r'^(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\s+\d{1,2},?\s+\d{4}$',
        r'^\d{4}$',
        r'^(?:fiscal\s+year|fy)\s+\d{4}$'
    ]
    
    if any(re.match(pattern, heading, re.IGNORECASE) for pattern in date_patterns):
        return True
        
    # Ignore headings that start with specific phrases
    ignore_starts = [
        'the following table',
        'the following tables',
        'notes to',
        'note:',
        'notes:',
        'continued from',
        'continued on',
        'amount',
        'amounts',
        'reported as',
        'current portion',
        'balance at',
        'balance as of',
        'year ended',
        'period ended',
        'none',
        'for example',
        'such as',
        'including',
        'this section',
        'discussion of'
    ]
    
    for start in ignore_starts:
        if heading.startswith(start):
            return True
            
    # Ignore if it's just parenthetical content
    if heading.startswith('(') and heading.endswith(')'):
        return True
        
    # Ignore if it's just a separator
    if set(heading).issubset({'-', '_', '=', '*', '.'}):
        return True
        
    # Ignore headings that end with a colon
    if heading.endswith(':'):
        return True
        
    return False

def split_by_subheadings(text: str) -> List[Dict[str, str]]:
    """Split text into chunks based on markdown headings that are not TOC or Part headers."""
    chunks = []
    current_lines = []
    current_subheading = None
    in_table = False
    table_buffer = []
    
    lines = text.split('\n')
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        
        # Check for table start/end
        if line.startswith('|'):
            in_table = True
            table_buffer.append(lines[i])
        elif in_table and not line:
            # Empty line after table - check if table is complete
            if i + 1 < len(lines) and not lines[i + 1].strip().startswith('|'):
                in_table = False
                current_lines.extend(table_buffer)
                table_buffer = []
            else:
                table_buffer.append(lines[i])
        elif in_table:
            table_buffer.append(lines[i])
        # Handle headings
        elif line.startswith('#'):
            heading_text = line.strip('#').strip()
            if not should_ignore_heading(heading_text):
                # If we have content from a previous subheading, save it
                if current_lines or table_buffer:
                    # Include any buffered table content
                    if table_buffer:
                        current_lines.extend(table_buffer)
                        table_buffer = []
                        in_table = False
                    
                    chunks.append({
                        "subheading": current_subheading,
                        "text": '\n'.join(current_lines).strip()
                    })
                    current_lines = []
                
                # Set the new subheading
                current_subheading = heading_text
            else:
                # If it's an ignored heading, treat it as regular content
                current_lines.append(lines[i])
        else:
            # Regular content line
            current_lines.append(lines[i])
        i += 1
    
    # Don't forget the last chunk and any remaining table content
    if table_buffer:
        current_lines.extend(table_buffer)
    if current_lines:
        chunks.append({
            "subheading": current_subheading,
            "text": '\n'.join(current_lines).strip()
        })
    
    return chunks

def save_chunks_to_file(chunks: List[Dict[str, Any]], output_file: str):
    """Save chunks to a file in a readable format."""
    # Create output directory if needed
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    # Write chunks to file
    with open(output_file, "w") as f:
        f.write("================================================================================\n")
        f.write(f"TOTAL CHUNKS: {len(chunks)}\n")
        f.write("================================================================================\n\n")
        
        for i, chunk in enumerate(chunks, 1):
            f.write(f"\n######################################## CHUNK {i} ########################################\n\n")
            
            # Write metadata
            f.write("METADATA:\n")
            f.write("--------------------\n")
            f.write(f"Item Heading: {chunk['heading']}\n")
            f.write(f"Subheading: {chunk['subheading']}\n")
            
            # Write content
            f.write("\nCONTENT:\n")
            f.write("--------------------\n")
            f.write(chunk['text'])
            f.write("\n\n================================================================================\n")

def assign_missing_item_headings(chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Post-process chunks to assign missing item headings based on the next known item heading.
    """
    # First find the index of each standard heading in our ordered list
    heading_indices = {heading: i for i, heading in enumerate(STANDARD_10K_HEADINGS)}
    
    # Find the first chunk with a valid item heading
    next_heading = None
    next_heading_index = None
    
    # Scan forward to find the next valid item heading
    for i in range(len(chunks)):
        if chunks[i]['heading'] in heading_indices:
            next_heading = chunks[i]['heading']
            next_heading_index = i
            break
    
    # If we found a valid next heading, assign missing headings
    if next_heading and next_heading_index is not None:
        # Get the index of this heading in our standard list
        next_idx = heading_indices[next_heading]
        if next_idx > 0:
            # The previous heading in our standard list should be assigned to all
            # chunks between TOC and this heading
            prev_heading = STANDARD_10K_HEADINGS[next_idx - 1]
            for i in range(1, next_heading_index):  # Start from 1 to skip TOC
                if not chunks[i]['heading']:  # Only assign if heading is missing
                    chunks[i]['heading'] = prev_heading
    
    return chunks

def is_table_row(line: str) -> bool:
    """Check if a line is part of a markdown table."""
    return bool(line.strip().startswith('|') and line.strip().endswith('|'))

def count_tokens(text: str) -> int:
    """
    Estimate the number of tokens in a text.
    This is a simple approximation - actual token count may vary with the tokenizer.
    """
    # Simple approximation: split on whitespace and punctuation
    words = text.split()
    # Assume average of 1.3 tokens per word for multilingual text
    return int(len(words) * 1.3)

def split_chunk_preserve_tables(chunk: Dict[str, str], max_tokens: int = 300) -> List[Dict[str, str]]:
    """
    Split a chunk if it exceeds max_tokens while preserving table structures.
    Returns a list of new chunks, each with the same heading/subheading as the original.
    Prefers splitting at paragraph breaks and avoids splitting after colons.
    """
    # Skip empty chunks
    if not chunk['text'].strip():
        return []
        
    if count_tokens(chunk['text']) <= max_tokens:
        return [chunk]
    
    new_chunks = []
    paragraphs = chunk['text'].split('\n\n')
    current_paragraphs = []
    current_tokens = 0
    in_table = False
    table_lines = []
    
    i = 0
    while i < len(paragraphs):
        paragraph = paragraphs[i]
        
        # Skip empty paragraphs or separator-only paragraphs
        if not paragraph.strip() or paragraph.strip() == '---':
            i += 1
            continue
            
        # Check if this paragraph contains a table
        lines = paragraph.split('\n')
        if any(is_table_row(line) for line in lines):
            # Handle table paragraph
            table_tokens = count_tokens(paragraph)
            if current_tokens + table_tokens > max_tokens and current_paragraphs:
                # Create new chunk with accumulated paragraphs
                new_chunks.append({
                    'heading': chunk.get('heading'),
                    'subheading': chunk.get('subheading'),
                    'text': '\n\n'.join(current_paragraphs).strip()
                })
                current_paragraphs = []
                current_tokens = 0
            current_paragraphs.append(paragraph)
            current_tokens += table_tokens
        else:
            # Regular paragraph
            paragraph_tokens = count_tokens(paragraph)
            
            # Check if this paragraph ends with a colon
            ends_with_colon = paragraph.rstrip().endswith(':')
            
            # If adding this paragraph would exceed the limit
            if current_tokens + paragraph_tokens > max_tokens:
                if current_paragraphs:
                    # Don't split if last paragraph ended with a colon
                    last_para = current_paragraphs[-1] if current_paragraphs else ""
                    if last_para.rstrip().endswith(':'):
                        # Try to include this paragraph even if it exceeds the limit
                        current_paragraphs.append(paragraph)
                    else:
                        # Create new chunk with accumulated paragraphs
                        new_chunks.append({
                            'heading': chunk.get('heading'),
                            'subheading': chunk.get('subheading'),
                            'text': '\n\n'.join(current_paragraphs).strip()
                        })
                        current_paragraphs = [paragraph]
                        current_tokens = paragraph_tokens
                else:
                    # If a single paragraph is too large, we need to split it by sentences
                    sentences = re.split(r'(?<=[.!?])\s+', paragraph)
                    current_sentence_group = []
                    current_sentence_tokens = 0
                    
                    for j, sentence in enumerate(sentences):
                        sentence_tokens = count_tokens(sentence)
                        
                        # Check if this sentence ends with a colon
                        ends_with_colon = sentence.rstrip().endswith(':')
                        
                        if current_sentence_tokens + sentence_tokens > max_tokens and current_sentence_group:
                            # Don't split if last sentence ended with a colon
                            if not current_sentence_group[-1].rstrip().endswith(':'):
                                new_chunks.append({
                                    'heading': chunk.get('heading'),
                                    'subheading': chunk.get('subheading'),
                                    'text': ' '.join(current_sentence_group).strip()
                                })
                                current_sentence_group = []
                                current_sentence_tokens = 0
                        current_sentence_group.append(sentence)
                        current_sentence_tokens += sentence_tokens
                    
                    if current_sentence_group:
                        current_paragraphs = [' '.join(current_sentence_group)]
                        current_tokens = current_sentence_tokens
            else:
                current_paragraphs.append(paragraph)
                current_tokens += paragraph_tokens
                
                # If this paragraph ends with a colon, try to include the next paragraph
                if ends_with_colon and i + 1 < len(paragraphs):
                    next_paragraph = paragraphs[i + 1]
                    next_tokens = count_tokens(next_paragraph)
                    if current_tokens + next_tokens <= max_tokens * 1.2:  # Allow slight overflow
                        current_paragraphs.append(next_paragraph)
                        current_tokens += next_tokens
                        i += 1  # Skip the next paragraph since we included it
        
        i += 1
    
    # Don't forget the last chunk
    if current_paragraphs:
        new_chunks.append({
            'heading': chunk.get('heading'),
            'subheading': chunk.get('subheading'),
            'text': '\n\n'.join(current_paragraphs).strip()
        })
    
    # Filter out any empty chunks
    return [chunk for chunk in new_chunks if chunk['text'].strip()]

def is_related_table_content(text: str) -> bool:
    """
    Determine if a text block is related to table content by checking for:
    - Table rows
    - Financial statement line items
    - Numeric data
    - Table metadata
    """
    # Skip empty text
    if not text.strip():
        return False
        
    lines = text.strip().split('\n')
    
    # Quick check for obvious table markers
    has_table_marker = False
    table_row_count = 0
    header_separator_found = False
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # Count table rows and check for header separator
        if line.startswith('|') and line.endswith('|'):
            table_row_count += 1
            has_table_marker = True
            if re.match(r'\|[\s\-:]+\|', line):
                header_separator_found = True
                
    # If we have a well-formed table (header, separator, and data rows)
    if has_table_marker and header_separator_found and table_row_count >= 3:
        return True
        
    # Check for financial statement patterns
    financial_patterns = [
        # Currency amounts
        r'\$\s*\d+(?:,\d{3})*(?:\.\d+)?',
        # Parenthetical amounts
        r'\(\$?\s*\d+(?:,\d{3})*(?:\.\d+)?\)',
        # Common financial statement headers
        r'(?i)^(?:assets|liabilities|equity|revenue|expenses|income|cash flows)',
        # Date ranges
        r'(?i)(?:year|quarter|period)s?\s+ended?\s+(?:january|february|march|april|may|june|july|august|september|october|november|december)\s+\d{1,2},\s+\d{4}',
        # Notes references
        r'\([0-9A-Z]\)',
        # Financial metrics
        r'(?i)(?:total|net|gross|operating|consolidated)\s+(?:revenue|income|loss|assets|liabilities|equity|earnings)'
    ]
    
    # Count how many lines match financial patterns
    financial_line_count = 0
    for line in lines:
        if any(re.search(pattern, line) for pattern in financial_patterns):
            financial_line_count += 1
            
    # If more than 30% of non-empty lines contain financial patterns
    non_empty_lines = len([l for l in lines if l.strip()])
    if non_empty_lines > 0 and financial_line_count / non_empty_lines > 0.3:
        return True
        
    return False

def should_merge_table_chunks(current_chunk: Dict[str, str], next_chunk: Dict[str, str]) -> bool:
    """
    Determine if two chunks should be merged based on their content and context.
    """
    # If they don't have the same item heading, don't merge
    if current_chunk.get('heading') != next_chunk.get('heading'):
        return False
        
    # If either chunk is empty, don't merge
    if not current_chunk.get('text').strip() or not next_chunk.get('text').strip():
        return False
        
    current_text = current_chunk['text']
    next_text = next_chunk['text']
    
    # Check if both chunks contain table-related content
    current_is_table = is_related_table_content(current_text)
    next_is_table = is_related_table_content(next_text)
    
    # If both are table-related, check if they're part of the same context
    if current_is_table and next_is_table:
        # If they share similar column headers, they're likely related
        current_headers = extract_table_headers(current_text)
        next_headers = extract_table_headers(next_text)
        if current_headers and next_headers and headers_are_similar(current_headers, next_headers):
            return True
            
        # If they're close together and have similar structure
        if len(current_text.split('\n')) < 20 and len(next_text.split('\n')) < 20:
            return True
            
        # If one appears to be a continuation of the other
        if is_table_continuation(current_text, next_text):
            return True
    
    # If one is a small chunk that looks like a table header or footer
    if len(current_text.split('\n')) < 4 or len(next_text.split('\n')) < 4:
        # Check if it contains table-related metadata
        metadata_patterns = [
            r'\([Ii]n (?:thousands|millions|billions)',
            r'[Nn]otes? to',
            r'[Cc]ontinued',
            r'[Yy]ear[s]? [Ee]nded',
            r'[Aa]s of',
            r'[Ss]ee accompanying notes'
        ]
        small_chunk = current_text if len(current_text.split('\n')) < 4 else next_text
        if any(re.search(pattern, small_chunk) for pattern in metadata_patterns):
            return True
    
    return False

def extract_table_headers(text: str) -> List[str]:
    """Extract column headers from a markdown table."""
    lines = text.strip().split('\n')
    for i, line in enumerate(lines):
        if line.startswith('|') and line.endswith('|'):
            headers = [col.strip() for col in line.strip('|').split('|')]
            # Check next line for separator
            if i + 1 < len(lines) and re.match(r'\|[\s\-:]+\|', lines[i + 1]):
                return headers
    return []

def headers_are_similar(headers1: List[str], headers2: List[str]) -> bool:
    """Check if two sets of table headers are similar."""
    if not headers1 or not headers2:
        return False
        
    # Normalize headers
    def normalize(headers):
        return [re.sub(r'\s+', ' ', h.lower().strip()) for h in headers if h.strip()]
        
    h1 = normalize(headers1)
    h2 = normalize(headers2)
    
    # If they're exactly the same
    if h1 == h2:
        return True
        
    # If one is a subset of the other
    if all(h in h1 for h in h2) or all(h in h2 for h in h1):
        return True
        
    # If they share significant overlap
    common = set(h1) & set(h2)
    return len(common) >= min(len(h1), len(h2)) * 0.5

def is_table_continuation(text1: str, text2: str) -> bool:
    """Check if text2 appears to be a continuation of text1's table."""
    lines1 = text1.strip().split('\n')
    lines2 = text2.strip().split('\n')
    
    # If either is empty, not a continuation
    if not lines1 or not lines2:
        return False
        
    # Check if the last line of text1 and first line of text2 have similar structure
    last_line = lines1[-1].strip()
    first_line = lines2[0].strip()
    
    if last_line.startswith('|') and first_line.startswith('|'):
        # Count pipes to check if they have the same number of columns
        pipes1 = last_line.count('|')
        pipes2 = first_line.count('|')
        return pipes1 == pipes2
        
    return False

def merge_related_chunks(chunks: List[Dict[str, str]], max_tokens: int = 300, min_tokens: int = 100) -> List[Dict[str, str]]:
    """
    Merge chunks that are part of the same table context while respecting the token limit.
    Also ensures that small chunks (below min_tokens) are merged with previous chunks if they share the same Item heading.
    """
    if not chunks:
        return chunks
        
    merged_chunks = []
    current_chunk = chunks[0]
    
    for next_chunk in chunks[1:]:
        should_merge = False
        
        # Check if chunks share the same item heading
        same_heading = current_chunk.get('heading') == next_chunk.get('heading')
        
        # Calculate token counts
        current_tokens = count_tokens(current_chunk['text'])
        next_tokens = count_tokens(next_chunk['text'])
        combined_tokens = count_tokens(current_chunk['text'] + '\n' + next_chunk['text'])
        
        # Determine if we should merge based on various conditions
        if same_heading:
            # Always merge if either chunk is very small and combined is within limit
            if (current_tokens < min_tokens or next_tokens < min_tokens) and combined_tokens <= max_tokens:
                should_merge = True
            # Merge if they're related table content
            elif should_merge_table_chunks(current_chunk, next_chunk) and combined_tokens <= max_tokens:
                should_merge = True
            # Merge if they're both small and sequential
            elif current_tokens < min_tokens and next_tokens < min_tokens and combined_tokens <= max_tokens:
                should_merge = True
        
        if should_merge:
            # Merge the chunks
            current_chunk['text'] = current_chunk['text'] + '\n' + next_chunk['text']
            # Keep the first meaningful subheading if it exists
            if (not current_chunk.get('subheading') or 
                current_chunk.get('subheading', '').lower() in ['none', 'no heading']) and next_chunk.get('subheading'):
                current_chunk['subheading'] = next_chunk['subheading']
        else:
            # If we're not merging and the current chunk is too small,
            # try to merge it with the previous chunk in merged_chunks
            if current_tokens < min_tokens and merged_chunks and current_chunk.get('heading') == merged_chunks[-1].get('heading'):
                prev_combined_tokens = count_tokens(merged_chunks[-1]['text'] + '\n' + current_chunk['text'])
                if prev_combined_tokens <= max_tokens:
                    merged_chunks[-1]['text'] = merged_chunks[-1]['text'] + '\n' + current_chunk['text']
                else:
                    merged_chunks.append(current_chunk)
            else:
                merged_chunks.append(current_chunk)
            current_chunk = next_chunk
    
    # Handle the last chunk
    if current_chunk:
        current_tokens = count_tokens(current_chunk['text'])
        if current_tokens < min_tokens and merged_chunks and current_chunk.get('heading') == merged_chunks[-1].get('heading'):
            # Try to merge with the previous chunk if they share the same heading
            prev_combined_tokens = count_tokens(merged_chunks[-1]['text'] + '\n' + current_chunk['text'])
            if prev_combined_tokens <= max_tokens:
                merged_chunks[-1]['text'] = merged_chunks[-1]['text'] + '\n' + current_chunk['text']
            else:
                merged_chunks.append(current_chunk)
        else:
            merged_chunks.append(current_chunk)
    
    return merged_chunks

def post_process_chunks(chunks: List[Dict[str, str]], max_tokens: int = 300) -> List[Dict[str, str]]:
    """
    Post-process chunks to ensure they don't exceed the token limit while preserving tables.
    Now also merges related table chunks while respecting the token limit.
    """
    # First split chunks that are too large
    processed_chunks = []
    for chunk in chunks:
        processed_chunks.extend(split_chunk_preserve_tables(chunk, max_tokens))
    
    # Then merge related table chunks while respecting the token limit
    processed_chunks = merge_related_chunks(processed_chunks, max_tokens)
    
    # Final verification that no chunk exceeds the limit
    final_chunks = []
    for chunk in processed_chunks:
        if count_tokens(chunk['text']) > max_tokens:
            # If a chunk somehow still exceeds the limit, split it again
            final_chunks.extend(split_chunk_preserve_tables(chunk, max_tokens))
        else:
            final_chunks.append(chunk)
    
    return final_chunks

def process_and_save_chunks(text: str, output_file: str = None) -> List[Dict[str, Any]]:
    """Process text into chunks and save to file if output_file is provided."""
    print(f"Total lines: {len(text.splitlines())}")
    
    # Find where content starts (after TOC)
    content_start_idx = find_start_of_content(text)
    lines = text.split('\n')
    content = '\n'.join(lines[content_start_idx:])
    
    # Process chunks
    chunks = []
    current_chunk_lines = []
    current_item_heading = None
    found_forward_looking = False
    
    for line in content.split('\n'):
        line_strip = line.strip()
        
        # Check for Forward-Looking Statements
        if line_strip.lower() == "# forward-looking statements":
            found_forward_looking = True
            if not current_item_heading:
                current_item_heading = "Item 1. Business"
        
        # Check if this is a new Item heading
        item_heading_info = detect_item_heading(line_strip)
        
        if item_heading_info and item_heading_info['full_heading'] in STANDARD_10K_HEADINGS:
            # If this is a real item heading (not just TOC entry)
            if len(line_strip.split()) > 3:
                # Save previous chunk if exists
                if current_chunk_lines:
                    chunks.append({
                        "heading": current_item_heading or "Item 1. Business",
                        "subheading": None,
                        "text": '\n'.join(current_chunk_lines).strip()
                    })
                    current_chunk_lines = []
                
                # Start new chunk
                current_chunk_lines = [line]
                current_item_heading = item_heading_info["full_heading"]
                found_forward_looking = False
            else:
                # This is a TOC entry, skip it
                continue
        else:
            # If we're after Forward-Looking Statements but before next item heading
            if found_forward_looking and not current_item_heading:
                current_item_heading = "Item 1. Business"
            
            # Add to current chunk
            current_chunk_lines.append(line)
    
    # Don't forget the last chunk
    if current_chunk_lines:
        chunks.append({
            "heading": current_item_heading or "Item 1. Business",
            "subheading": None,
            "text": '\n'.join(current_chunk_lines).strip()
        })
    
    # Split chunks by subheadings
    final_chunks = []
    for chunk in chunks:
        subheading_chunks = split_by_subheadings(chunk['text'])
        for subchunk in subheading_chunks:
            final_chunks.append({
                "heading": chunk['heading'],
                "subheading": subchunk['subheading'],
                "text": subchunk['text']
            })
    
    # Post-process to ensure chunks don't exceed token limit
    final_chunks = post_process_chunks(final_chunks)
    
    # Save to file if output file is provided
    if output_file:
        save_chunks_to_file(final_chunks, output_file)
        print(f"\nChunks also saved to file: {output_file}")
    
    return final_chunks

def get_sample_text():
    """
    Returns a sample 10-K text for testing the chunking functionality.
    """
    return '''
    LICE L. JOLLA|Corporate Vice President and Chief Accounting Officer (Principal Accounting Officer)|
'''

def print_chunks(chunks):
    """
    Print chunks in a clear, readable format to the console.
    """
    print("\n" + "="*80)
    print(f"TOTAL CHUNKS: {len(chunks)}")
    print("="*80 + "\n")

    for i, chunk in enumerate(chunks, 1):
        print(f"\n{'#'*40} CHUNK {i} {'#'*40}")
        print("\nMETADATA:")
        print("-"*20)
        metadata = chunk['metadata']
        print(f"Item Heading: {metadata['item_heading']}")
        print(f"Keywords: {', '.join(metadata['keywords']) if metadata['keywords'] else 'None'}")
        print(f"Type: {metadata['type']}")
        print(f"Company: {metadata['company']}")
        print(f"Document Type: {metadata['document_type']}")
        
        print("\nCONTENT:")
        print("-"*20)
        print(chunk["content"])
        print("\n" + "="*80 + "\n")

def test():
    """Run a test with sample data."""
    sample_text = get_sample_text()
    
    # Generate output filename with timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = os.path.join("chunking_results", f"10k_chunks_{timestamp}.md")
    
    # Process chunks and save to file
    chunks = process_and_save_chunks(sample_text, output_file)
    
    # Print some stats about the chunks
    print("\n================================================================================")
    print(f"TOTAL CHUNKS: {len(chunks)}")
    print("================================================================================\n")
    
    # Print metadata for each chunk
    for i, chunk in enumerate(chunks, 1):
        print(f"\n######################################## CHUNK {i} ########################################\n")
        print("METADATA:")
        print("--------------------")
        print(f"Item Heading: {chunk['heading']}")
        print(f"Subheading: {chunk['subheading']}")
        print("\nCONTENT:")
        print("--------------------")
        print(chunk['text'])
        print("\n================================================================================\n")

def main():
    import sys
    print("Starting main function...")
    if len(sys.argv) < 2:
        print("Usage:")
        print("  Test with markdown file: python output_chunking.py <filename>.md")
        print("  Test with sample data:   python output_chunking.py --test")
        sys.exit(0)

    print(f"Arguments received: {sys.argv}")
    if sys.argv[1] == "--test":
        print("Running test mode...")
        test()
        return

    # Handle markdown file testing
    input_file = sys.argv[1]
    print(f"Processing input file: {input_file}")
    
    # Generate output filename with timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    base_name = os.path.splitext(os.path.basename(input_file))[0]
    output_file = os.path.join("chunking_results", f"{base_name}_chunks_{timestamp}.md")
    print(f"Output will be written to: {output_file}")
    
    # Read the input file
    try:
        with open(input_file, 'r') as f:
            content = f.read()
            print(f"Successfully read input file. Content length: {len(content)} characters")
    except Exception as e:
        print(f"Error reading input file: {e}")
        return
        
    # Process the content
    try:
        chunks = process_and_save_chunks(content, output_file)
        print(f"Successfully processed {len(chunks)} chunks")
    except Exception as e:
        print(f"Error processing chunks: {e}")
        return

if __name__ == "__main__":
    main()

---

<filepath>import_3_indexing.py</filepath>

import os
import sys
from typing import List, Dict, Any
from datetime import datetime
from import_2_chunking import process_and_save_chunks

try:
    from pinecone import Pinecone
except ImportError:
    raise ImportError("Please install Pinecone via: pip install pinecone")

def embed_and_upsert(
    text_content: str,
    metadata: Dict[str, str],
    api_key: str,
    save_chunks: bool = True,
    namespace: str = ""
) -> bool:
    """
    Embed chunks of text and upsert them to Pinecone.
    
    Args:
        text_content: The text content to process and embed
        metadata: Document metadata to include with each chunk
        api_key: Pinecone API key
        save_chunks: Whether to save chunks to a file
        namespace: Pinecone namespace to use (company-specific)
    
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        from pinecone import Pinecone
    except ImportError:
        print("Please install pinecone-client: pip install pinecone-client")
        return False

    # Initialize Pinecone
    pc = Pinecone(api_key=api_key)
    
    # Get the index name from environment variables
    index_name = os.getenv("PINECONE_INDEX_NAME", "financialdocs")
    
    # Get the index
    index = pc.Index(index_name)
    
    # Process text into chunks
    chunks = process_and_save_chunks(text_content)
    if not chunks:
        print("No chunks generated")
        return False
    
    print(f"\nEmbedding and upserting {len(chunks)} chunks to namespace: {namespace}")
    print(f"Using index: {index_name}")
    
    # Prepare vectors for batch upsert
    vectors = []
    for i, chunk in enumerate(chunks):
        # Generate embedding
        try:
            embedding_list = pc.inference.embed(
                model="multilingual-e5-large",
                inputs=[chunk['text']],
                parameters={"input_type": "passage", "truncate": "END"}
            )
            if not embedding_list or not embedding_list.data:
                print(f"Warning: No embedding generated for chunk {i}")
                continue
                
            embedding = embedding_list[0].values
            
            # Combine document metadata with chunk metadata
            chunk_metadata = {
                "company_name": metadata.get("company_name", ""),
                "fiscal_year": metadata.get("fiscal_year", ""),
                "document_type": metadata.get("document_type", ""),
                "document_url": metadata.get("document_url", ""),
                "top_level_heading": chunk.get("heading") or "",
                "subheading": chunk.get("subheading") or "",
                "chunk_text": chunk.get("text") or ""
            }
            
            # Add to vectors list
            vectors.append({
                "id": f"{metadata.get('company_name', 'unknown')}_{metadata.get('fiscal_year', 'unknown')}_{i}",
                "values": embedding,
                "metadata": chunk_metadata
            })
            
        except Exception as e:
            print(f"Error processing chunk {i}: {str(e)}")
            continue
    
    if not vectors:
        print("No vectors generated")
        return False
    
    # Batch upsert to Pinecone
    try:
        # Upsert in batches of 100
        batch_size = 100
        for i in range(0, len(vectors), batch_size):
            batch = vectors[i:i + batch_size]
            index.upsert(vectors=batch, namespace=namespace)
            print(f"Upserted batch {i//batch_size + 1} of {(len(vectors) + batch_size - 1)//batch_size}")
        
        print(f"Successfully indexed {len(vectors)} vectors in namespace: {namespace}")
        return True
        
    except Exception as e:
        print(f"Error upserting to Pinecone: {str(e)}")
        return False

def main():
    if len(sys.argv) < 2:
        print("Usage: python indexing.py <input_file> [--metadata key=value ...]")
        return

    input_file = sys.argv[1]
    
    # Parse metadata from command line arguments
    metadata = {}
    i = 2
    while i < len(sys.argv):
        if sys.argv[i] == "--metadata" and i + 1 < len(sys.argv):
            key_value = sys.argv[i + 1].split("=")
            if len(key_value) == 2:
                metadata[key_value[0]] = key_value[1]
            i += 2
        else:
            i += 1

    # Ensure required metadata fields
    required_fields = ['company_name', 'fiscal_year', 'document_type']
    missing_fields = [field for field in required_fields if field not in metadata]
    
    if missing_fields:
        print(f"Missing required metadata fields: {', '.join(missing_fields)}")
        print("Please provide all required metadata using --metadata key=value")
        return

    # Get API key
    api_key = os.getenv("PINECONE_API_KEY")
    if not api_key:
        api_key = input("Enter your Pinecone API key: ").strip()
        if not api_key:
            print("No API key provided. Aborting.")
            return

    try:
        # Read the input file
        with open(input_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Process and index the content
        embed_and_upsert(content, metadata, api_key)
        
    except FileNotFoundError:
        print(f"Error: File '{input_file}' not found")
    except Exception as e:
        print(f"Error processing file: {str(e)}")

if __name__ == "__main__":
    main()

---

<filepath>import_pipeline.py</filepath>

import os
from typing import Optional, Dict
from datetime import datetime
from dotenv import load_dotenv
import re

# Ensure required packages are installed
required_packages = {
    'python-dotenv': 'python-dotenv',
    'exa-py': 'exa-py',
    'llama-parse': 'llama-parse',
    'pinecone-client': 'pinecone-client'
}

def check_and_install_packages():
    import subprocess
    import sys
    
    for package, pip_name in required_packages.items():
        try:
            __import__(package.replace('-', '_'))
        except ImportError:
            print(f"Installing required package: {package}")
            subprocess.check_call([sys.executable, "-m", "pip", "install", pip_name])

# Install required packages
check_and_install_packages()

# Import our pipeline components
from import_0_search import search_10k, setup_logging
from import_1_parse import download_sec_filing_pdf, parse_pdf_to_markdown
from import_2_chunking import process_and_save_chunks
from import_3_indexing import embed_and_upsert

def generate_namespace(company_name: str) -> str:
    """
    Generate a clean namespace name from company name.
    Follows Pinecone's namespace naming conventions:
    - Lowercase alphanumeric characters and hyphens only
    - Must start with a letter
    - Maximum length of 63 characters
    """
    # Convert to lowercase and replace spaces/special chars with hyphens
    namespace = re.sub(r'[^a-zA-Z0-9-]', '-', company_name.lower())
    # Remove consecutive hyphens
    namespace = re.sub(r'-+', '-', namespace)
    # Remove leading/trailing hyphens
    namespace = namespace.strip('-')
    # Ensure it starts with a letter
    if not namespace[0].isalpha():
        namespace = 'company-' + namespace
    # Truncate to 63 characters if needed
    return namespace[:63]

def run_pipeline(company_name: str) -> bool:
    """
    Run the full document retrieval and processing pipeline:
    1. Search for company's 10-K using Exa
    2. Download PDF from SEC and parse to markdown
    3. Process markdown into chunks
    4. Embed and index chunks in Pinecone using company-specific namespace
    
    Args:
        company_name: Name of the company to process
    
    Returns:
        bool: True if pipeline completed successfully, False otherwise
    """
    # Setup logging
    setup_logging()
    print(f"\nStarting pipeline for company: {company_name}")
    
    # Load environment variables
    load_dotenv()
    
    # Get required API keys
    exa_api_key = os.getenv("EXA_API_KEY")
    sec_api_key = os.getenv("SEC_API_KEY")
    llama_api_key = os.getenv("LLAMA_CLOUD_API_KEY")
    pinecone_api_key = os.getenv("PINECONE_API_KEY")
    
    # Validate API keys
    missing_keys = []
    if not exa_api_key: missing_keys.append("EXA_API_KEY")
    if not sec_api_key: missing_keys.append("SEC_API_KEY")
    if not llama_api_key: missing_keys.append("LLAMA_CLOUD_API_KEY")
    if not pinecone_api_key: missing_keys.append("PINECONE_API_KEY")
    
    if missing_keys:
        print(f"Missing required API keys: {', '.join(missing_keys)}")
        print("Please add them to your .env file")
        return False
    
    # Generate namespace for this company
    namespace = generate_namespace(company_name)
    print(f"Using namespace: {namespace}")

    try:
        # Step 1: Search for 10-K
        print("\n=== STEP 1: Searching for 10-K ===")
        metadata = search_10k(company_name, exa_api_key)
        if not metadata:
            print("Failed to find 10-K document. Pipeline stopped.")
            return False
        
        # Step 2: Download and Parse
        print("\n=== STEP 2: Downloading and Parsing PDF ===")
        pdf_path = download_sec_filing_pdf(metadata["document_url"], sec_api_key)
        if not pdf_path:
            print("Failed to download PDF. Pipeline stopped.")
            return False
            
        markdown_path = parse_pdf_to_markdown(pdf_path, llama_api_key)
        if not markdown_path:
            print("Failed to parse PDF to markdown. Pipeline stopped.")
            return False
            
        # Step 3: Process into chunks
        print("\n=== STEP 3: Processing into chunks ===")
        with open(markdown_path, 'r', encoding='utf-8') as f:
            markdown_content = f.read()
            
        # Generate output filename for chunks
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        chunks_file = f"chunking_results/{company_name}_chunks_{timestamp}.md"
        
        chunks = process_and_save_chunks(markdown_content, chunks_file)
        if not chunks:
            print("Failed to process chunks. Pipeline stopped.")
            return False
            
        print(f"Successfully processed {len(chunks)} chunks")
        
        # Step 4: Embed and Index
        print("\n=== STEP 4: Embedding and Indexing ===")
        embed_and_upsert(
            text_content=markdown_content,
            metadata=metadata,
            api_key=pinecone_api_key,
            save_chunks=True,
            namespace=namespace  # Add company-specific namespace
        )
        
        print("\n=== Pipeline completed successfully! ===")
        print(f"- PDF saved to: {pdf_path}")
        print(f"- Markdown saved to: {markdown_path}")
        print(f"- Chunks saved to: {chunks_file}")
        print(f"- Data indexed in Pinecone namespace: {namespace}")
        
        return True
        
    except Exception as e:
        print(f"\nError in pipeline: {str(e)}")
        return False

def main():
    """Run the pipeline with command line arguments or prompt."""
    import sys
    
    if len(sys.argv) > 1:
        company_name = sys.argv[1]
    else:
        company_name = input("Enter company name (e.g., NVIDIA, Apple, Microsoft): ").strip()
    
    if not company_name:
        print("Company name cannot be empty")
        return
        
    success = run_pipeline(company_name)
    if not success:
        print("\nPipeline failed. Check the error messages above.")
    
if __name__ == "__main__":
    main() 

---

<filepath>analysis_module_plan.py</filepath>

import os
import google.generativeai as genai
from typing import TypedDict, Optional, List, Dict, Any
from langgraph.graph.state import StateGraph, START, END
from langchain_core.runnables import RunnableConfig
from langgraph.types import Command
import json
from datetime import datetime

# ----------------------------------------------------------------------------
# 1. State Definition
# ----------------------------------------------------------------------------

class AnalysisState(TypedDict):
    """
    Holds the entire state for the initial user input + planning module.
    """

    user_query: str
    company_name: str
    analysis_type: Optional[str]
    information_needs: Optional[List[Dict[str, str]]]
    execution_plan: Optional[List[Dict[str, Any]]]  
    plan_raw: Optional[Dict[str, Any]]

# ----------------------------------------------------------------------------
# 2. Context Cache (Workflow + Modules Overview)
# ----------------------------------------------------------------------------

CONTEXT_CACHE = """
## TASKCONTEXT
You are an expert financial analysis agent with deep expertise in finance, accounting, financial modeling, financial statements analysis and financial documentation and document structures.
You are a deliberate and comprehensive structured thinker and planner for comprehensive financial analysis.

## TASK
Your task is to plan a comprehensive analysis based on a user's query about a company's financial data and performance. 
You will create a structured plan that outlines the information gathering needs and subsequent analysis steps.
The plan must be thorough, logically sequenced, and follow all specified constraints.

## WORKFLOW CONTEXT
The analysis process is divided into distinct modules:
1. User Input Processing (Current)
2. Analysis Planning (Current - Your Focus)
3. Information Gathering (Separate Module)
4. Analysis Execution (Separate Module)

## BACKGROUND: DOCUMENT STR
The 10-K filing follows a standardized structure with specific items containing distinct types of information:

### Core Business Information
- **Item 1. Business**: Company operations, products/services, revenue streams, competitive landscape, market share, industry trends, key customers, suppliers, seasonality, regulations, IP, R&D, geographic markets, corporate structure
- **Item 1A. Risk Factors**: Major business risks including economic, market, operational, financial, regulatory, cybersecurity, environmental, competition, litigation risks
- **Item 1B. Unresolved Staff Comments**: SEC staff comments on previous filings

### Assets and Legal
- **Item 2. Properties**: Physical assets, facilities, real estate, manufacturing locations, distribution centers
- **Item 3. Legal Proceedings**: Ongoing litigation, regulatory proceedings, patent disputes, settlements
- **Item 4. Mine Safety Disclosures**: Mine safety information if applicable

### Financial Performance
- **Item 5. Market for Registrant's Common Equity**: Stock information, dividends, repurchase programs
- **Item 6. [Reserved]**: Currently reserved by SEC
- **Item 7. Management's Discussion and Analysis**: Business overview, performance indicators, liquidity, operations results, future outlook, trends
- **Item 7A. Market Risk Disclosures**: Analysis of market risks (interest rate, currency, commodity price risks)
- **Item 8. Financial Statements**: Complete audited financial statements, footnotes, accounting policies
- **Item 9. Changes in and Disagreements with Accountants**: Accounting changes and disputes
- **Item 9A. Controls and Procedures**: Internal control assessment
- **Item 9B. Other Information**: Additional material information
- **Item 9C. Foreign Jurisdictions Disclosure**: Information about operations in jurisdictions preventing PCAOB inspections

### Governance and Management
- **Item 10. Directors and Executive Officers**: Leadership details, governance policies
- **Item 11. Executive Compensation**: Detailed compensation information
- **Item 12. Security Ownership**: Major shareholders information
- **Item 13. Related Transactions**: Related party transactions
- **Item 14. Principal Accountant Fees**: Accounting services and fees
- **Item 15. Exhibits**: Additional documentation and certifications
- **Item 16. Form 10-K Summary**: Optional report summary

### Common Analysis Patterns

Here are some common analysis planning patterns that you can adapt to the specific user query for formulating your plan.

1. **Revenue Analysis Pattern**
   - **Core Information Needs**:
     - Historical revenue data by segment/product line
     - Revenue growth rates and trends
     - Geographic distribution of revenue
     - Revenue concentration (key customers/products)
   - **Supporting Information Needs**:
     - Market size and share data
     - Pricing strategies and changes
     - Seasonality patterns
     - Currency exposure impacts
   - **Analysis Components**:
     - Segment contribution analysis
     - Year-over-year growth decomposition
     - Revenue quality assessment
     - Forward-looking indicators

2. **Risk Assessment Pattern**
   - **Core Information Needs**:
     - Identified risk factors and their potential impact
     - Risk mitigation strategies
     - Historical risk events and outcomes
     - Industry-specific risk factors
   - **Supporting Information Needs**:
     - Control measures and their effectiveness
     - Insurance and hedging strategies
     - Regulatory compliance status
     - Pending litigation or disputes
   - **Analysis Components**:
     - Risk severity and likelihood matrix
     - Year-over-year risk evolution
     - Comparative industry risk assessment
     - Risk response effectiveness

3. **Growth Analysis Pattern**
   - **Core Information Needs**:
     - Strategic growth initiatives
     - Market expansion plans
     - R&D investments and pipeline
     - Capital allocation strategy
   - **Supporting Information Needs**:
     - Competitive landscape changes
     - Market penetration metrics
     - Historical execution success
     - Resource availability
   - **Analysis Components**:
     - Growth driver identification
     - Investment return analysis
     - Market opportunity sizing
     - Execution capability assessment

4. **Competitive Position Pattern**
   - **Core Information Needs**:
     - Market share data and trends
     - Competitive advantages
     - Product/service differentiation
     - Cost position and efficiency metrics
   - **Supporting Information Needs**:
     - Industry structure and dynamics
     - Technology and innovation position
     - Brand strength indicators
     - Customer relationship metrics
   - **Analysis Components**:
     - Competitive advantage sustainability
     - Market position trajectory
     - Capability gap analysis
     - Strategic response assessment

5. **Financial Health Pattern**
   - **Core Information Needs**:
     - Key financial metrics and ratios
     - Cash flow composition
     - Capital structure
     - Working capital efficiency
   - **Supporting Information Needs**:
     - Credit ratings and debt covenants
     - Off-balance sheet obligations
     - Liquidity sources
     - Asset quality indicators
   - **Analysis Components**:
     - Financial flexibility assessment
     - Stress test scenarios
     - Capital efficiency analysis
     - Sustainability of financial position

6. **Operational Efficiency Pattern**
   - **Core Information Needs**:
     - Operating margins and trends
     - Cost structure breakdown
     - Productivity metrics
     - Capacity utilization
   - **Supporting Information Needs**:
     - Supply chain performance
     - Quality metrics
     - Employee productivity
     - Asset performance data
   - **Analysis Components**:
     - Margin driver analysis
     - Operational bottleneck identification
     - Efficiency improvement potential
     - Cost optimization opportunities

### Key Information Interconnections
1. **Operational Performance Analysis**
   - Item 7 (MD&A) → Item 8 (Financial Statements)
   - Item 1 (Business Operations) → Item 2 (Operational Assets)
   - Item 1A (Risks) → Item 7 (Risk Mitigation Strategies)

2. **Strategic Analysis**
   - Item 1 (Strategy) → Item 7 (Implementation Progress)
   - Item 1A (Risks) → Item 9A (Control Measures)
   - Item 2 (Assets) → Item 8 (Asset Utilization)

3. **Financial Health Assessment**
   - Item 8 (Financials) → Item 7 (Management's Interpretation)
   - Item 7A (Risks) → Item 7 (Risk Management)
   - Item 1A (Risks) → Item 5 (Market Impact)

### Information Depth Guide
1. **Quantitative Data (High Precision)**
   - Item 8: Detailed financial statements, exact figures
   - Item 5: Specific share counts, prices, dividends
   - Item 11: Precise compensation figures

2. **Qualitative Analysis (Medium-High Detail)**
   - Item 7: Detailed management discussion, trends
   - Item 1: Comprehensive business description
   - Item 1A: Extensive risk descriptions

3. **Contextual Information (Medium Detail)**
   - Item 2: Asset descriptions and locations
   - Item 10: Management backgrounds
   - Item 13: Related party descriptions

4. **Supporting Information (Variable Detail)**
   - Item 9B: Supplementary information
   - Item 15: Referenced documents
   - Item 16: Optional summary

## DATA SOURCES
Primary Source: 10-K Financial Documents
- Stored in Pinecone vector database
- Documents are chunked with metadata tags
- Tags include: top-level item headings, subheadings
- Contains both numerical data and textual analysis

## PLANNING INSTRUCTIONS
Follow these steps to create a comprehensive analysis plan:

1. Query Analysis
   - Deeply understand the user's request
   - Identify key analysis components needed
   - Determine if request needs factual data, qualitative analysis, or both

2. Information Requirements
   - List all required data points and information
   - Distinguish between factual data and broad analysis needs

3. Analysis Structure
   - Break down analysis into logical steps
   - Ensure steps build upon each other
   - Include data processing, calculations, and interpretations

4. Output Planning
   - Define how results should be presented
   - Specify any visualizations or comparisons needed
   - Plan clear and actionable conclusions

## JSON OUTPUT SCHEMA
{
  "analysisType": "string - Categorize the type of analysis (e.g., 'Revenue Analysis', 'Growth Assessment')",
  "informationNeeds": [
    {
      "summary": "Brief description of needed information",
      "description": "Detailed explanation of data format and context needed",
      "tag": "factual | broad",
      "stored": "unique_identifier_for_retrieval"
    }
  ],
  "executionPlan": [
    {
      "stepName": "Clear, descriptive name of the analysis step",
      "actions": [
        {
          "actionType": "FinancialCalculation | TextAnalysis | ComparisonAnalysis | ScenarioModeling | WebResearch | ResultFormatting",
          "task": "Specific instructions for this action",
          "information": ["stored_identifiers_from_information_needs"]
        }
      ]
    }
  ]
}

## CONSTRAINTS
1. Return ONLY valid JSON - no additional text or markdown
2. Final step MUST be ResultFormatting type
3. Do not include specific retrieval queries
4. Ensure all stored identifiers are unique
5. All steps must be logically connected

## QUALITY CHECKS
- Are all required data points identified?
- Is the execution plan logically sequenced?
- Are dependencies between steps clear?
- Is the final output format well-defined?
- Have potential challenges been noted?
"""

# ----------------------------------------------------------------------------
# 3. Configure Gemini & (Optional) Context Caching
# ----------------------------------------------------------------------------

API_KEY = "AIzaSyAp3nWo75vwNQ_e0aUGNDo2qwK_HGKtliI"  # Replace with your actual Gemini API key
genai.configure(api_key=API_KEY)

def create_context_cache():
    """
    Placeholder for advanced caching usage if you want to store large doc(s)
    in a context-based approach. Not implementing in detail here.
    """
    return None

def get_planning_model():
    model = genai.GenerativeModel(
        model_name="gemini-1.5-flash",
        generation_config=genai.types.GenerationConfig(
            temperature=0.2,
            max_output_tokens=2500,
        )
    )

    # Define the response schema for structured output
    model.response_schema = {
        "type": "object",
        "properties": {
            "analysisType": {"type": "string"},
            "informationNeeds": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "summary": {"type": "string"},
                        "description": {"type": "string"},
                        "tag": {
                            "type": "string",
                            "enum": ["factual", "broad"]
                        },
                        "stored": {"type": "string"}
                    },
                    "required": ["summary", "description", "tag", "stored"]
                }
            },
            "executionPlan": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "stepName": {"type": "string"},
                        "actions": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "actionType": {
                                        "type": "string",
                                        "enum": [
                                            "FinancialCalculation",
                                            "TextAnalysis",
                                            "ComparisonAnalysis",
                                            "ScenarioModeling",
                                            "WebResearch",
                                            "ResultFormatting"
                                        ]
                                    },
                                    "task": {"type": "string"},
                                    "information": {
                                        "type": "array",
                                        "items": {"type": "string"}
                                    }
                                },
                                "required": ["actionType", "task", "information"]
                            }
                        }
                    },
                    "required": ["stepName", "actions"]
                }
            }
        },
        "required": [
            "analysisType",
            "informationNeeds",
            "executionPlan"
        ]
    }
    
    return model

# ----------------------------------------------------------------------------
# 4. Planning Node
# ----------------------------------------------------------------------------

def planning_node(state: AnalysisState, config: RunnableConfig, use_context_cache: bool = True) -> AnalysisState:
    """
    Module 2 - Planning:
    1. Accept user_query & company_name from state
    2. Optionally provide the big context (CONTEXT_CACHE) to the LLM
    3. Call Gemini to produce a JSON-based plan
    4. Parse & store each portion into state
    """
    user_query = state.get("user_query", "")
    company_name = state.get("company_name", "")
    if not user_query or not company_name:
        state["plan_raw"] = {
            "error": "Missing user_query or company_name",
            "raw_response": None
        }
        return state

    # Build prompt
    context_part = CONTEXT_CACHE if use_context_cache else ""
    prompt = f"""
{context_part}

User's query: '{user_query}'
Company name: '{company_name}'

Return a JSON object following the schema provided. Do not include any markdown formatting or extra text.
"""

    model = get_planning_model()

    raw_text = ""
    try:
        response = model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=0.0,
                max_output_tokens=1500,
            )
        )
        raw_text = response.text.strip()
        
        # Remove any markdown formatting if present
        if raw_text.startswith("```"):
            raw_text = raw_text.split("```")[1]
            # If there's a "json" fence, remove it
            if raw_text.startswith("json"):
                raw_text = raw_text[4:]
        raw_text = raw_text.strip()
        
        # Parse JSON
        parsed = json.loads(raw_text)
        
        # Store the full plan in plan_raw
        state["plan_raw"] = parsed

        # Distribute fields into separate state variables
        state["analysis_type"] = parsed.get("analysisType")
        state["information_needs"] = parsed.get("informationNeeds")
        state["execution_plan"] = parsed.get("executionPlan")

    except Exception as e:
        state["plan_raw"] = {
            "error": f"Error processing response: {str(e)}",
            "raw_response": raw_text
        }

    return state

# ----------------------------------------------------------------------------
# 5. Graph Setup
# ----------------------------------------------------------------------------

def build_initial_planning_graph(use_context_cache: bool = True):
    """
    Builds a simple LangGraph workflow:
      START -> planning_node -> END
    """
    builder = StateGraph(AnalysisState)

    def planning_node_wrapper(state: AnalysisState, config: RunnableConfig) -> AnalysisState:
        return planning_node(state, config, use_context_cache=use_context_cache)

    builder.add_node("planning_node", planning_node_wrapper)

    builder.add_edge(START, "planning_node")
    builder.add_edge("planning_node", END)

    return builder.compile()

def run_planning_workflow(user_query: str, company_name: str, use_context_cache: bool = True) -> dict:
    """
    High-level function that sets up the graph state with user inputs,
    then runs the planning node.
    """
    graph = build_initial_planning_graph(use_context_cache=use_context_cache)
    initial_state: AnalysisState = {
        "user_query": user_query,
        "company_name": company_name,
        "analysis_type": None,
        "information_needs": None,
        "execution_plan": None,
        "plan_raw": None
    }
    final_state = graph.invoke(initial_state)
    return final_state

def save_plan_as_markdown(state: AnalysisState, output_file: str = None) -> str:
    """
    Save the analysis plan as a well-formatted markdown file.
    Returns the path to the saved file.
    """
    # Create output directory if it doesn't exist
    output_dir = "analysis_plans"
    os.makedirs(output_dir, exist_ok=True)

    # Generate default output name if none provided
    if not output_file:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        company_name = state.get("company_name", "unknown").replace(" ", "_")
        output_file = os.path.join(output_dir, f"analysis_plan_{company_name}_{timestamp}.md")

    # Format the content
    content = []
    content.append(f"# Analysis Plan for {state['company_name']}\n")
    content.append(f"## Query\n{state['user_query']}\n")
    
    if state.get("analysis_type"):
        content.append(f"## Analysis Type\n{state['analysis_type']}\n")
    
    if state.get("information_needs"):
        content.append("## Information Needs\n")
        for i, need in enumerate(state["information_needs"], 1):
            content.append(f"### {i}. {need['summary']}")
            content.append(f"- **Description**: {need['description']}")
            content.append(f"- **Type**: {need['tag']}")
            content.append(f"- **Storage ID**: `{need['stored']}`\n")
    
    if state.get("execution_plan"):
        content.append("## Execution Plan\n")
        for i, step in enumerate(state["execution_plan"], 1):
            content.append(f"### Step {i}: {step['stepName']}")
            for j, action in enumerate(step["actions"], 1):
                content.append(f"#### Action {j}")
                content.append(f"- **Type**: {action['actionType']}")
                content.append(f"- **Task**: {action['task']}")
                if action.get("information"):
                    content.append("- **Required Information**:")
                    for info in action["information"]:
                        content.append(f"  - `{info}`")
                content.append("")

    # Write to file
    with open(output_file, "w", encoding="utf-8") as f:
        f.write("\n".join(content))

    return output_file

# ----------------------------------------------------------------------------
# 6. Testing / Main
# ----------------------------------------------------------------------------

if __name__ == "__main__":
    test_query = "Analyze Amazon's revenue distribution across regions and business segments."
    test_company = "Amazon"
    result = run_planning_workflow(test_query, test_company, use_context_cache=True)
    print("\n===== Analysis Planning Output =====")
    print(f"\nQuery: {test_query}")
    print(f"Company: {test_company}\n")
    
    # Save as markdown
    output_file = save_plan_as_markdown(result)
    print(f"\nAnalysis plan saved to: {output_file}")
    
    # Also print JSON for debugging
    plan = result.get("plan_raw", {})
    formatted_json = json.dumps(plan, indent=2)
    print("\nRaw JSON output:")
    print(formatted_json)

---

<filepath>analysis_module_infogathering_orchestrator.py</filepath>

import os
from typing import TypedDict, Any, List, Dict
from langgraph.graph.state import StateGraph, START, END
from langchain_core.runnables import RunnableConfig

# We'll define a simple state type for information gathering
class InfoGatheringState(TypedDict):
    # The output from planning: list of information needs
    information_needs: List[Dict[str, str]]  # each has {summary, description, tag, stored}
    # We'll store results in a dictionary keyed by 'stored' field
    gathered_results: Dict[str, str]

def gather_info_for_all_needs(state: InfoGatheringState, config: RunnableConfig) -> InfoGatheringState:
    """
    Orchestrates the data gathering for each of the informationNeeds.
    This function can call a sub-graph (like vectorDB retrieval) for each requirement.
    For demonstration, we'll just pass the 'information_needs' to a subgraph call (analysis_module_vectordb).
    """
    from analysis_module_vectordb import build_vectordb_retrieval_graph

    results_storage = {}
    # Initialize the vectorDB retrieval graph
    retrieval_graph = build_vectordb_retrieval_graph()

    for info_need in state["information_needs"]:
        # Build sub-state for the retrieval
        sub_state = {
            "info_need_summary": info_need["summary"],
            "info_need_description": info_need["description"],
            "info_need_tag": info_need["tag"],
            "info_need_stored_key": info_need["stored"],
            "retrieved_text": ""
        }

        # Invoke the vector db retrieval sub-graph
        final_sub_state = retrieval_graph.invoke(sub_state)
        # Store the retrieved_text in results_storage
        results_storage[info_need["stored"]] = final_sub_state.get("retrieved_text", "")
    
    # Update the main state with all collected results
    state["gathered_results"] = results_storage
    return state

def build_info_gathering_orchestration_graph() -> StateGraph:
    """
    Builds a simple graph that collects info for each item using a sub-graph.
    """
    from analysis_module_vectordb import build_vectordb_retrieval_graph  # Just referencing

    builder = StateGraph(InfoGatheringState)
    builder.add_node("gather_info_for_all_needs", gather_info_for_all_needs)

    builder.add_edge(START, "gather_info_for_all_needs")
    builder.add_edge("gather_info_for_all_needs", END)

    return builder.compile()

def run_information_gathering_workflow(information_needs: List[Dict[str, str]]) -> dict:
    """
    High-level function that sets up the graph state with the information needs,
    then runs the gather_info_for_all_needs node. Returns all gathered results.
    """
    graph = build_info_gathering_orchestration_graph()
    initial_state: InfoGatheringState = {
        "information_needs": information_needs,
        "gathered_results": {}
    }
    final_state = graph.invoke(initial_state)
    return final_state

if __name__ == "__main__":
    # Example usage
    example_information_needs = [
        {
            "summary": "Total revenue for last fiscal year",
            "description": "Exact figure, with reference to official statement",
            "tag": "factual",
            "stored": "last_fiscal_revenue"
        },
        {
            "summary": "Discussion of major risk factors",
            "description": "A broad overview of the main risk factor items",
            "tag": "broad",
            "stored": "risk_factors_overview"
        }
    ]

    results = run_information_gathering_workflow(example_information_needs)
    print("\nGathered Results:")
    for key, value in results["gathered_results"].items():
        print(f"{key}: {value}")

---

<filepath>analysis_module_vectordb.py</filepath>

import os
import json
import logging
from datetime import datetime
from typing import TypedDict, Optional, List, Dict, Any
from langfuse import Langfuse
from langfuse.decorators import observe, langfuse_context
import google.generativeai as genai
from pinecone import Pinecone

# Configure logging to write to both file and console
def setup_logging():
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"logs/vectordb_analysis_{timestamp}.log"
    
    # Create a formatter that includes timestamp and level
    formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # Configure file handler
    file_handler = logging.FileHandler(log_filename)
    file_handler.setFormatter(formatter)
    
    # Configure console handler
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    
    # Get the root logger and set its level
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    
    # Remove any existing handlers and add our new ones
    root_logger.handlers = []
    root_logger.addHandler(file_handler)
    root_logger.addHandler(console_handler)
    
    logging.info(f"Logging initialized. Log file: {log_filename}")

langfuse = Langfuse(
    secret_key="sk-lf-b1e8e8fb-1d04-4296-be27-d876a788f874",
    public_key="pk-lf-d5c6afef-cf5c-43ae-9296-54061f1f3587",
    host="https://cloud.langfuse.com"
)

from langgraph.graph.state import StateGraph, START, END
from langchain_core.runnables import RunnableConfig
from langgraph.types import Send, Command

# Standard 10-K item headings and approximate info stored
TENK_ITEM_HEADINGS = {
    "Item 1. Business": "Description of business operations, main products and services, revenue streams, competitive landscape, market share, industry trends, key customers, suppliers, seasonality, government regulations, intellectual property, research and development, employees, geographic markets, corporate structure, subsidiaries, company history, business strategy, recent acquisitions or divestitures",
    "Item 1A. Risk Factors": "Details about major risks and uncertainties for the business, economic risks, market risks, operational risks, financial risks, regulatory risks, cybersecurity risks, environmental risks, competition risks, litigation risks, reputational risks, supply chain risks, technological risks, geopolitical risks, pandemic-related risks",
    "Item 1B. Unresolved Staff Comments": "Any unresolved comments or issues raised by the SEC staff on the company's previous filings",
    "Item 2. Properties": "Information about property, plants, and equipment, real estate holdings, leased properties, manufacturing facilities, distribution centers, office locations, land ownership, property values, capacity utilization",
    "Item 3. Legal Proceedings": "Any ongoing legal matters or litigation, pending lawsuits, regulatory proceedings, environmental litigation, patent disputes, class action suits, settlement agreements",
    "Item 4. Mine Safety Disclosures": "Mine safety information if applicable, violations, citations, legal actions related to mine safety",
    "Item 5. Market for Registrant's Common Equity": "Stock market details, holder information, dividends, stock price history, stock repurchase programs, equity compensation plans, securities authorized for issuance, performance graph comparing stock performance to market indices",
    "Item 6. None": "This item is currently reserved by the SEC for future use",
    "Item 7. Management's Discussion and Analysis of Financial Condition and Results of Operations": "Management analysis, business overview, future outlook, key performance indicators, liquidity and capital resources, results of operations, critical accounting estimates, off-balance sheet arrangements, contractual obligations, market risk disclosures, segment reporting, trends affecting the business",
    "Item 7A. Quantitative and Qualitative Disclosures About Market Risk": "Analysis of market risks, including interest rate risk, foreign currency risk, commodity price risk, equity price risk, credit risk, hedging activities",
    "Item 8. Financial Statements and Supplementary Data": "Audited financial statements, including Balance Sheet, Income Statement, Statement of Cash Flows, Statement of Stockholders' Equity, footnotes, accounting policies, segment information, quarterly financial data, five-year financial summary",
    "Item 9. Changes in and Disagreements with Accountants": "Accounting or auditing-related changes, disagreements with previous auditors, reasons for auditor change",
    "Item 9A. Controls and Procedures": "Internal control, disclosure control procedures, management's assessment of internal controls, attestation report of the registered public accounting firm",
    "Item 9B. Other Information": "Miscellaneous or additional info not otherwise captured, subsequent events, material information not reported on Form 8-K",
    "Item 9C. Disclosure Regarding Foreign Jurisdictions that Prevent Inspections": "Information about operating in jurisdictions that prevent PCAOB inspections",
    "Item 10. Directors, Executive Officers and Corporate Governance": "Details about executives, board of directors, governance policies, code of ethics, board committees, corporate governance guidelines, director independence, executive biographies",
    "Item 11. Executive Compensation": "Information on compensation of officers and directors, salary, bonuses, stock awards, option grants, pension plans, employment agreements, compensation discussion and analysis (CD&A)",
    "Item 12. Security Ownership of Certain Beneficial Owners": "Information on major shareholders, ownership structure, beneficial ownership table, equity compensation plan information",
    "Item 13. Certain Relationships and Related Transactions": "Details of related party transactions or conflicts of interest, transactions with management or major shareholders, policies for approval of related party transactions",
    "Item 14. Principal Accountant Fees and Services": "Accounting fees, services performed by principal accountant, audit fees, audit-related fees, tax fees, all other fees, pre-approval policies for auditor services",
    "Item 15. Exhibits, Financial Statement Schedules": "Exhibits, schedules attached to the 10-K, list of subsidiaries, list of guarantor subsidiaries, consent of independent registered public accounting firm, power of attorney, certifications required under Sarbanes-Oxley Act",
    "Item 16. Form 10-K Summary": "Optional summary of the report if provided, key highlights from each section"
}

"""
This module sets up a graph that:
1) Takes a single 'raw_information_needs' string from the planning module,
   parses multiple info needs, then for each info need runs a pipeline of:
   - gemini_determine_headings
   - retrieve_metadata_with_zero_vector
   - configure_search_with_gemini
   - pinecone_vector_search
   (all in parallel for each info need)
2) Once all parallel tasks are done, interpret_results_with_gemini is invoked
   to produce final_interpretation, with one paragraph per info need.

Each node is decorated with @observe.
We inline code from agent_retrieval, referencing functions but not importing.
"""

###############################################################################
# State Definitions
###############################################################################

class InfoNeedItem(TypedDict):
    info_need: str
    headings: List[str]
    metadata: Dict[str, Any]        # from zero-vector retrieval
    search_config: Dict[str, Any]   # from configure search
    search_results: List[Dict[str, Any]]  # from pinecone search

class AnalysisVectorDBState(TypedDict):
    raw_information_needs: str
    info_need_items: List[InfoNeedItem]
    done_count: int
    final_interpretation: str
    credentials_valid: bool
    credentials_error: Optional[str]
    credentials: Dict[str, str]
    parallel_status: Dict[int, str]  # Track status of each parallel branch
    error_states: Dict[int, List[str]]  # Track errors in parallel branches
    stream_events: List[Dict[str, Any]]  # Track streaming events from parallel branches
    company_namespace: str  # The namespace to use for Pinecone queries

###############################################################################
# Node 0: validate_credentials
###############################################################################
@observe()
def validate_credentials(
    state: AnalysisVectorDBState,
    config: RunnableConfig
) -> Any:
    """
    Node #0: Validate all required API keys and credentials before starting the pipeline.
    """
    required_credentials = {
        "GEMINI_API_KEY": os.getenv("GEMINI_API_KEY"),
        "PINECONE_API_KEY": os.getenv("PINECONE_API_KEY")
    }
    
    missing_credentials = [
        key for key, value in required_credentials.items()
        if not value
    ]
    
    if missing_credentials:
        error_msg = f"Missing required credentials: {', '.join(missing_credentials)}"
        logging.error(error_msg)
        state["credentials_valid"] = False
        state["credentials_error"] = error_msg
        return Command(
            update={},
            goto=END
        )
    
    # Store validated credentials in state for reuse
    state["credentials_valid"] = True
    state["credentials"] = required_credentials
    
    # Preserve the company namespace in the state update
    return Command(
        update={
            "credentials_valid": True,
            "credentials": required_credentials,
            "company_namespace": state.get("company_namespace")
        },
        goto="parse_information_needs"
    )

###############################################################################
# Node 1: parse_information_needs
###############################################################################
@observe()
def parse_information_needs(
    state: AnalysisVectorDBState,
    config: RunnableConfig
) -> Any:
    """
    Splits the 'raw_information_needs' field into multiple info needs.
    Then populates state['info_need_items'] = [ {info_need: <need>} ... ]
    """
    logging.info("Parsing multiple information needs from raw input.")
    raw = state.get("raw_information_needs", "")
    
    # Split on semicolons and clean up
    splitted = [s.strip() for s in raw.split(";") if s.strip()]
    
    info_need_items = []
    for i, s in enumerate(splitted):
        # Initialize each item with empty collections
        info_need_items.append({
            "info_need": s,
            "headings": [],
            "metadata": {},
            "search_config": {"heading_configs": []},
            "search_results": []
        })
        # Initialize tracking for this item
        state["parallel_status"][i] = "pending"
        state["error_states"][i] = []

    state["info_need_items"] = info_need_items
    state["done_count"] = 0  # reset
    
    n_items = len(info_need_items)
    logging.info(f"Found {n_items} information needs to process:")
    for i, item in enumerate(info_need_items):
        logging.info(f"\n{i+1}. {item['info_need'][:100]}...")

    if n_items == 0:
        # If none, we can jump directly to interpret
        return Command(
            update={"info_need_items": []},
            goto="interpret_results_with_gemini"
        )

    # Return command to continue to next node
    return Command(
        update={
            "info_need_items": info_need_items,
            "company_namespace": state.get("company_namespace")  # Preserve namespace
        },
        goto="gemini_determine_headings"
    )

###############################################################################
# Node 2: gemini_determine_headings
###############################################################################
@observe()
def gemini_determine_headings(
    state: AnalysisVectorDBState,
    config: RunnableConfig
) -> Any:
    """
    Node #2: Use Gemini to interpret each info need and map it to relevant 10-K headings.
    We'll store them in info_need_items[item_index]["headings"].
    """
    gemini_api_key = os.getenv("GEMINI_API_KEY")
    if not gemini_api_key:
        logging.error("Missing GEMINI_API_KEY environment variable.")
        # Mark all as failed and continue
        for i in range(len(state["info_need_items"])):
            state["error_states"][i].append("Missing GEMINI_API_KEY")
        return Command(
            update={},
            goto="retrieve_metadata_with_zero_vector"
        )

    # Configure gemini
    genai.configure(api_key=gemini_api_key)
    model = genai.GenerativeModel("gemini-1.5-flash")

    # Process each info need
    for item_index, item in enumerate(state["info_need_items"]):
        info_need = item["info_need"]
        logging.info(f"\nProcessing info need {item_index + 1}: {info_need[:100]}...")

        # Build a prompt that includes the standard headings and the info need
        headings_text = "\n".join([f"{k}: {v}" for k, v in TENK_ITEM_HEADINGS.items()])
        prompt = (
            "You are a helpful AI assistant. The user is asking about 10-K information. \n"
            "Below are the standard 10-K item headings and rough descriptions:\n"
            f"{headings_text}\n\n"
            "User query:\n"
            f"{info_need}\n\n"
            "Instructions:\n"
            "1. Analyze the query and identify the TOP 3 MOST RELEVANT item headings.\n"
            "2. Return ONLY a valid JSON object with exactly this structure:\n"
            "{\n"
            '    "relevant_headings": [\n'
            '        "Item X. Exact Heading Name",\n'
            '        "Item Y. Exact Heading Name",\n'
            '        "Item Z. Exact Heading Name"\n'
            "    ]\n"
            "}\n\n"
            "Requirements:\n"
            "- Include EXACTLY 3 headings\n"
            "- Each heading must be an EXACT MATCH from the list above\n"
            "- Return ONLY the JSON object, no other text\n"
            "- Ensure the JSON is properly formatted with double quotes"
        )

        try:
            response = model.generate_content(
                contents=prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.2,
                    max_output_tokens=700
                )
            )
            
            # Parse the JSON response from the text
            try:
                response_text = response.text.strip()
                # If response is wrapped in ```json and ```, remove them
                if response_text.startswith('```json'):
                    response_text = response_text[7:]
                if response_text.startswith('```'):
                    response_text = response_text[3:]
                if response_text.endswith('```'):
                    response_text = response_text[:-3]
                
                structured_response = json.loads(response_text.strip())
                relevant_headings = structured_response.get("relevant_headings", [])
                
                # Ensure headings are recognized
                recognized_headings = [
                    h.strip() for h in relevant_headings 
                    if h.strip() in TENK_ITEM_HEADINGS
                ][:3]  # Limit to 3 even if somehow more are returned

                logging.info("\nRELEVANT ITEM HEADINGS:")
                for h in recognized_headings:
                    logging.info(f"- {h}")
                
                state["info_need_items"][item_index]["headings"] = recognized_headings
                
            except json.JSONDecodeError as je:
                logging.error(f"Failed to parse JSON response for item {item_index}: {je}")
                state["error_states"][item_index].append(f"JSON parse error: {je}")
                state["info_need_items"][item_index]["headings"] = []
                
        except Exception as e:
            logging.error(f"Error during heading determination for item {item_index}: {e}")
            state["error_states"][item_index].append(f"Gemini error: {e}")
            state["info_need_items"][item_index]["headings"] = []

    return Command(
        update={
            "info_need_items": state["info_need_items"],
            "company_namespace": state.get("company_namespace")  # Preserve namespace
        },
        goto="retrieve_metadata_with_zero_vector"
    )

###############################################################################
# Node 3: retrieve_metadata_with_zero_vector
###############################################################################
@observe()
def retrieve_metadata_with_zero_vector(
    state: AnalysisVectorDBState,
    config: RunnableConfig
) -> Any:
    """
    Node #3: Use a zero vector query to retrieve available subheadings for each relevant heading.
    We'll store the result in info_need_items[item_index]["metadata"].
    """
    pinecone_api_key = os.getenv("PINECONE_API_KEY")
    if not pinecone_api_key:
        logging.error("Missing PINECONE_API_KEY environment variable.")
        # Mark all as failed and continue
        for i in range(len(state["info_need_items"])):
            state["error_states"][i].append("Missing PINECONE_API_KEY")
            state["info_need_items"][i]["metadata"] = {}
        return Command(
            update={},
            goto="configure_search_with_gemini"
        )

    pc = Pinecone(api_key=pinecone_api_key)
    index = pc.Index(host="https://financialdocs-ij61u7y.svc.aped-4627-b74a.pinecone.io")
    
    # Debug logging for state inspection
    logging.info("\nState contents:")
    for key, value in state.items():
        if key != "info_need_items":  # Skip long arrays
            logging.info(f"- {key}: {value}")
    
    # Get the namespace from the state - it should be set in the test case
    namespace = state.get("company_namespace")
    if not namespace:
        logging.error("No company namespace found in state")
        logging.error(f"Available state keys: {list(state.keys())}")
        # Mark all as failed and continue
        for i in range(len(state["info_need_items"])):
            state["error_states"][i].append("Missing company namespace")
            state["info_need_items"][i]["metadata"] = {}
        return Command(
            update={},
            goto="configure_search_with_gemini"
        )

    # Create a zero vector with same dimensions as our embeddings (1024 for multilingual-e5-large)
    zero_vector = [0.0] * 1024

    # Process each info need
    for item_index, item in enumerate(state["info_need_items"]):
        logging.info(f"\nRetrieving metadata for info need {item_index + 1}")
        available_subheadings = {}
        headings = item["headings"]

        logging.info("\nSUBHEADINGS BY ITEM:")
        for heading in headings:
            try:
                metadata_filter = {
                    "top_level_heading": heading
                }
                
                results = index.query(
                    namespace=namespace,
                    vector=zero_vector,
                    top_k=100,
                    include_values=False,
                    include_metadata=True,
                    filter=metadata_filter
                )
                
                logging.info(f"\n{heading}:")
                if not results or "matches" not in results:
                    logging.info("└── No subheadings found")
                    available_subheadings[heading] = []
                    continue

                matches = results.get("matches", [])
                subheadings = set()
                for match in matches:
                    meta = match.get("metadata", {})
                    subheading = meta.get("subheading", "").strip()
                    if subheading:
                        subheadings.add(subheading)
                
                available_subheadings[heading] = sorted(list(subheadings))
                
                if subheadings:
                    for i, sh in enumerate(sorted(subheadings), 1):
                        prefix = "└──" if i == len(subheadings) else "├──"
                        logging.info(f"{prefix} {sh}")
                else:
                    logging.info("└── No subheadings found")

            except Exception as e:
                logging.error(f"Error retrieving metadata for {heading}: {e}")
                state["error_states"][item_index].append(f"Metadata error for {heading}: {e}")
                available_subheadings[heading] = []
                continue

        # Store the metadata in the state
        state["info_need_items"][item_index]["metadata"] = {
            "available_subheadings": available_subheadings,
            "retrieved_for_headings": headings
        }

    return Command(
        update={},
        goto="configure_search_with_gemini"
    )

###############################################################################
# Node 4: configure_search_with_gemini
###############################################################################
@observe()
def configure_search_with_gemini(
    state: AnalysisVectorDBState,
    config: RunnableConfig
) -> Any:
    """
    Node #4: Use Gemini to analyze each query and available metadata to configure the search.
    The search strategy combines:
    1. First two item headings without subheading filters (for broader context)
    2. Up to two very specific heading + subheading combinations (for targeted results)
    """
    gemini_api_key = os.getenv("GEMINI_API_KEY")
    if not gemini_api_key:
        logging.error("Missing GEMINI_API_KEY environment variable.")
        # Mark all as failed and continue with basic config
        for i, item in enumerate(state["info_need_items"]):
            state["error_states"][i].append("Missing GEMINI_API_KEY")
            state["info_need_items"][i]["search_config"] = {
                "heading_configs": [
                    {"heading": h, "use_subheading_filter": False}
                    for h in item["headings"][:2]
                ]
            }
        return Command(
            update={},
            goto="pinecone_vector_search"
        )

    genai.configure(api_key=gemini_api_key)
    model = genai.GenerativeModel("gemini-1.5-flash")

    # Process each info need
    for item_index, item in enumerate(state["info_need_items"]):
        logging.info(f"\nConfiguring search for info need {item_index + 1}")
        
        # Get first two headings for broad context searches
        broad_headings = item["headings"][:2]
        
        # Build context about available metadata
        metadata = item["metadata"]
        available_subheadings = metadata.get("available_subheadings", {})
        
        metadata_context = "Available headings and subheadings:\n"
        for heading, subheadings in available_subheadings.items():
            metadata_context += f"\n{heading}:\n"
            for subheading in subheadings:
                metadata_context += f"  - {subheading}\n"

        info_need = item["info_need"]
        prompt = (
            "You are a search configuration expert. Given a user query and available metadata fields, "
            "determine up to TWO very specific heading + subheading combinations that are most relevant.\n\n"
            f"User Query: {info_need}\n\n"
            f"{metadata_context}\n"
            "Instructions:\n"
            "1. Analyze the query and available metadata.\n"
            "2. Identify up to TWO very specific heading + subheading combinations that are HIGHLY relevant.\n"
            "3. Return a JSON object with this structure:\n"
            "{\n"
            '    "specific_searches": [\n'
            "        {\n"
            '            "heading": "Item X...",\n'
            '            "subheading": "Exact Subheading Name"\n'
            "        },\n"
            "        ...\n"
            "    ]\n"
            "}\n\n"
            "IMPORTANT:\n"
            "- Return 0-2 specific searches (only if highly confident)\n"
            "- Each subheading must EXACTLY match one from the available ones\n"
            "- Only include combinations that are HIGHLY likely to contain relevant information\n"
            "- Quality over quantity - better to return fewer, more precise matches\n"
        )

        try:
            response = model.generate_content(
                contents=prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.2,
                    max_output_tokens=1000
                )
            )
            
            # Parse the JSON response
            try:
                response_text = response.text.strip()
                if response_text.startswith('```json'):
                    response_text = response_text[7:]
                if response_text.startswith('```'):
                    response_text = response_text[3:]
                if response_text.endswith('```'):
                    response_text = response_text[:-3]
                
                specific_searches = json.loads(response_text.strip())
                
                # Build final search configuration
                search_config = {"heading_configs": []}
                
                # Add broad context searches (first two headings without subheading filters)
                for heading in broad_headings:
                    search_config["heading_configs"].append({
                        "heading": heading,
                        "use_subheading_filter": False
                    })
                
                # Add specific searches with subheading filters
                for specific in specific_searches.get("specific_searches", []):
                    heading = specific.get("heading")
                    subheading = specific.get("subheading")
                    
                    # Validate the combination exists in our available metadata
                    if (heading in available_subheadings and 
                        subheading in available_subheadings[heading]):
                        search_config["heading_configs"].append({
                            "heading": heading,
                            "use_subheading_filter": True,
                            "relevant_subheadings": [subheading]
                        })
                
                logging.info("\nSEARCH CONFIGURATION:")
                # Log broad context searches
                logging.info("\nBroad context searches:")
                for config in search_config["heading_configs"][:2]:
                    logging.info(f"- {config['heading']} (no subheading filter)")
                
                # Log specific searches
                specific_searches = search_config["heading_configs"][2:]
                if specific_searches:
                    logging.info("\nSpecific searches:")
                    for config in specific_searches:
                        if config.get("use_subheading_filter") and config.get("relevant_subheadings"):
                            logging.info(f"- {config['heading']}")
                            logging.info(f"  Subheading: {config['relevant_subheadings'][0]}")
                
                state["info_need_items"][item_index]["search_config"] = search_config
                
            except json.JSONDecodeError as je:
                logging.error(f"Failed to parse search configuration for item {item_index}")
                state["error_states"][item_index].append(f"JSON parse error: {je}")
                state["info_need_items"][item_index]["search_config"] = {
                    "heading_configs": [
                        {"heading": h, "use_subheading_filter": False}
                        for h in broad_headings
                    ]
                }
                
        except Exception as e:
            logging.error(f"Error during search configuration for item {item_index}")
            state["error_states"][item_index].append(f"Gemini error: {e}")
            state["info_need_items"][item_index]["search_config"] = {
                "heading_configs": [
                    {"heading": h, "use_subheading_filter": False}
                    for h in broad_headings
                ]
            }

    return Command(
        update={
            "info_need_items": state["info_need_items"],
            "company_namespace": state.get("company_namespace")  # Preserve namespace
        },
        goto="pinecone_vector_search"
    )

###############################################################################
# Node 5: pinecone_vector_search
###############################################################################
@observe()
def pinecone_vector_search(
    state: AnalysisVectorDBState,
    config: RunnableConfig
) -> Any:
    """
    Node #5: Perform semantic search in Pinecone with metadata filtering based on LLM configuration.
    We'll store the results in info_need_items[item_index]["search_results"].
    """
    pinecone_api_key = os.getenv("PINECONE_API_KEY")
    if not pinecone_api_key:
        logging.error("Missing PINECONE_API_KEY environment variable.")
        # Mark all as failed and continue
        for i in range(len(state["info_need_items"])):
            state["error_states"][i].append("Missing PINECONE_API_KEY")
            state["info_need_items"][i]["search_results"] = []
        return Command(
            update={},
            goto="subgraph_item_done"
        )

    pc = Pinecone(api_key=pinecone_api_key)
    index = pc.Index(host="https://financialdocs-ij61u7y.svc.aped-4627-b74a.pinecone.io")
    
    # Get the namespace from the state - it should be set in the test case
    namespace = state.get("company_namespace")
    if not namespace:
        logging.error("No company namespace found in state")
        # Mark all as failed and continue
        for i in range(len(state["info_need_items"])):
            state["error_states"][i].append("Missing company namespace")
            state["info_need_items"][i]["search_results"] = []
        return Command(
            update={},
            goto="subgraph_item_done"
        )

    # Process each info need
    for item_index, item in enumerate(state["info_need_items"]):
        logging.info(f"\nPerforming vector search for info need {item_index + 1}")
        all_chunks = []
        info_need = item["info_need"]
        search_config = item.get("search_config", {"heading_configs": []})

        # Create one embedding for the user query
        try:
            query_embedding_list = pc.inference.embed(
                model="multilingual-e5-large",
                inputs=[info_need],
                parameters={"input_type": "query", "truncate": "END"}
            )
        except Exception as e:
            logging.error(f"Error embedding query for item {item_index}: {e}")
            state["error_states"][item_index].append(f"Embedding error: {e}")
            state["info_need_items"][item_index]["search_results"] = []
            continue

        if not query_embedding_list or not query_embedding_list.data:
            logging.warning(f"No query embedding returned for item {item_index}")
            state["error_states"][item_index].append("No embedding returned")
            state["info_need_items"][item_index]["search_results"] = []
            continue

        query_vector = query_embedding_list[0].values
        logging.info(f"Embedding generated for query: {info_need[:80]}...")

        # Search based on configuration
        for heading_config in search_config.get("heading_configs", []):
            try:
                heading = heading_config.get("heading")
                use_subheading_filter = heading_config.get("use_subheading_filter", False)
                relevant_subheadings = heading_config.get("relevant_subheadings", [])

                # Build metadata filter
                metadata_filter = {
                    "top_level_heading": heading
                }

                # Add subheading filter if configured
                if use_subheading_filter and relevant_subheadings:
                    metadata_filter["subheading"] = {"$in": relevant_subheadings}
                    logging.info(f"Searching {heading} with subheading filter: {relevant_subheadings}")
                else:
                    logging.info(f"Searching {heading} without subheading filter")
                
                results = index.query(
                    namespace=namespace,
                    vector=query_vector,
                    top_k=5,
                    include_values=False,
                    include_metadata=True,
                    filter=metadata_filter
                )
                
                if not results or "matches" not in results:
                    logging.warning(f"No results for heading: {heading}")
                    continue

                matches = results["matches"] or []
                if matches:
                    logging.info(f"Found {len(matches)} matches for {heading}")
                
                for match in matches:
                    meta = match.get("metadata", {})
                    chunk_info = {
                        "text": meta.get("chunk_text", ""),
                        "heading": meta.get("top_level_heading", ""),
                        "subheading": meta.get("subheading", ""),
                        "score": match.get("score", 0)
                    }
                    if chunk_info["text"]:
                        all_chunks.append(chunk_info)

            except Exception as e:
                logging.error(f"Error during Pinecone query for heading {heading} in item {item_index}: {e}")
                state["error_states"][item_index].append(f"Search error for {heading}: {e}")
                continue

        # Store results for this item
        logging.info(f"\nFound {len(all_chunks)} relevant chunks for item {item_index}")
        state["info_need_items"][item_index]["search_results"] = all_chunks

    # Update completion status
    state["done_count"] = len(state["info_need_items"])
    for i in range(len(state["info_need_items"])):
        state["parallel_status"][i] = "completed"

    return Command(
        update={
            "info_need_items": state["info_need_items"],
            "done_count": len(state["info_need_items"]),
            "parallel_status": state["parallel_status"],
            "company_namespace": state.get("company_namespace")  # Preserve namespace
        },
        goto="subgraph_item_done"
    )

###############################################################################
# Node 6: subgraph_item_done
###############################################################################
@observe()
def subgraph_item_done(
    state: AnalysisVectorDBState,
    config: RunnableConfig
) -> Any:
    """
    Called after processing all items. We check if there were any errors
    and proceed to interpret_results_with_gemini.
    """
    # Log completion status
    logging.info("\nProcessing complete:")
    logging.info(f"Total items processed: {len(state['info_need_items'])}")
    
    # Check for errors
    items_with_errors = [
        i for i, errors in state["error_states"].items()
        if errors
    ]
    if items_with_errors:
        logging.warning(f"\nItems with errors: {len(items_with_errors)}")
        for item_index in items_with_errors:
            logging.warning(f"\nItem {item_index + 1} errors:")
            for error in state["error_states"][item_index]:
                logging.warning(f"- {error}")
    
    # Always proceed to interpretation
    return Command(
        update={
            "info_need_items": state["info_need_items"],
            "done_count": state["done_count"],
            "parallel_status": state["parallel_status"],
            "error_states": state["error_states"],
            "company_namespace": state.get("company_namespace")  # Preserve namespace
        },
        goto="interpret_results_with_gemini"
    )

###############################################################################
# Node 7: interpret_results_with_gemini
###############################################################################
@observe()
def interpret_results_with_gemini(
    state: AnalysisVectorDBState,
    config: RunnableConfig
) -> AnalysisVectorDBState:
    """
    Once all items are done, we produce final_interpretation with one paragraph per info need
    referencing the search results, headings, etc.
    """
    logging.info("interpret_results_with_gemini: combining results into final text.")

    paragraphs = []
    for i, item in enumerate(state["info_need_items"]):
        info_need = item["info_need"]
        results = item["search_results"]
        headings = item["headings"]
        metadata = item["metadata"]
        search_config = item["search_config"]
        errors = state["error_states"].get(i, [])

        # Build a comprehensive summary
        summary_lines = [
            f"Information Need {i+1}: {info_need}",
            f"\nStatus:",
            f"- Relevant headings identified: {len(headings)}",
            f"- Search configurations created: {len(search_config.get('heading_configs', []))}",
            f"- Results found: {len(results)}",
        ]

        if errors:
            summary_lines.append("\nErrors encountered:")
            for error in errors:
                summary_lines.append(f"- {error}")

        if headings:
            summary_lines.append("\nRelevant headings:")
            for h in headings:
                summary_lines.append(f"- {h}")

        if results:
            summary_lines.append("\nTop search results:")
            for j, result in enumerate(results[:3], 1):  # Show top 3 results
                summary_lines.extend([
                    f"\nResult {j}:",
                    f"Heading: {result['heading']}",
                    f"Subheading: {result['subheading']}",
                    f"Score: {result['score']:.3f}",
                    f"Text: {result['text'][:200]}..."  # Show first 200 chars
                ])
        else:
            summary_lines.append("\nNo search results found.")

        paragraphs.append("\n".join(summary_lines))

    final_text = "\n\n" + "="*80 + "\n\n".join(paragraphs)
    state["final_interpretation"] = final_text
    return state


###############################################################################
# Graph Construction
###############################################################################
def build_vectordb_retrieval_graph() -> StateGraph:
    """
    Build the main graph for handling multiple information needs in sequence.
    The flow is:
    1. validate_credentials
    2. parse_information_needs
    3. gemini_determine_headings (processes all items)
    4. retrieve_metadata_with_zero_vector (processes all items)
    5. configure_search_with_gemini (processes all items)
    6. pinecone_vector_search (processes all items)
    7. subgraph_item_done (checks completion)
    8. interpret_results_with_gemini
    """
    builder = StateGraph(AnalysisVectorDBState)
    
    # Add nodes
    builder.add_node("validate_credentials", validate_credentials)
    builder.add_node("parse_information_needs", parse_information_needs)
    builder.add_node("gemini_determine_headings", gemini_determine_headings)
    builder.add_node("retrieve_metadata_with_zero_vector", retrieve_metadata_with_zero_vector)
    builder.add_node("configure_search_with_gemini", configure_search_with_gemini)
    builder.add_node("pinecone_vector_search", pinecone_vector_search)
    builder.add_node("subgraph_item_done", subgraph_item_done)
    builder.add_node("interpret_results_with_gemini", interpret_results_with_gemini)

    # Add edges for sequential processing
    builder.add_edge(START, "validate_credentials")
    builder.add_edge("validate_credentials", "parse_information_needs")
    builder.add_edge("parse_information_needs", "gemini_determine_headings")
    builder.add_edge("gemini_determine_headings", "retrieve_metadata_with_zero_vector")
    builder.add_edge("retrieve_metadata_with_zero_vector", "configure_search_with_gemini")
    builder.add_edge("configure_search_with_gemini", "pinecone_vector_search")
    builder.add_edge("pinecone_vector_search", "subgraph_item_done")
    builder.add_edge("subgraph_item_done", "interpret_results_with_gemini")
    builder.add_edge("interpret_results_with_gemini", END)

    return builder.compile()


###############################################################################
# Quick test
###############################################################################
if __name__ == "__main__":
    # Initialize logging with our custom setup
    setup_logging()
    
    logging.info("Starting VectorDB Analysis Test Run")
    logging.info("====================================")

    # Test case configuration
    company_name = "amazon"
    # Generate clean namespace (lowercase alphanumeric only)
    import re
    namespace = re.sub(r'[^a-z0-9]', '', company_name.lower())[:63]  # Max 63 chars for Pinecone

    logging.info(f"Company: {company_name}")
    logging.info(f"Namespace: {namespace}")
    logging.info("====================================")

    # Structured information needs
    information_needs = {
        "analysisType": "Financial Growth Analysis",
        "informationNeeds": [
            {
                "summary": "Amazon's Revenue Streams",
                "description": "Identify and quantify Amazon's main revenue streams (e.g., North America e-commerce, International e-commerce, AWS, etc.) from the 10-K. Include data for the last 3 years.",
                "tag": "factual",
                "stored": "revenueStreamsData"
            },
            {
                "summary": "Amazon's Growth Projections",
                "description": "Extract any statements or projections regarding future growth potential from the Management Discussion and Analysis (MD&A) section of the 10-K.",
                "tag": "broad",
                "stored": "growthProjections"
            },
            {
                "summary": "Amazon's Risk Factors",
                "description": "Identify and summarize key risk factors that could impact future growth, as described in the 10-K's Risk Factors section.",
                "tag": "broad",
                "stored": "riskFactors"
            }
        ]
    }

    logging.info("Information Needs:")
    for i, need in enumerate(information_needs["informationNeeds"], 1):
        logging.info(f"\n{i}. {need['summary']}")
        logging.info(f"   Description: {need['description']}")
        logging.info(f"   Tag: {need['tag']}")

    logging.info("\n====================================")

    # Convert information needs to semicolon-separated string
    raw_information_needs = ";".join(
        item["description"] for item in information_needs["informationNeeds"]
    )

    # Initialize state with proper structure
    test_state: AnalysisVectorDBState = {
        "raw_information_needs": raw_information_needs,
        "info_need_items": [],
        "done_count": 0,
        "final_interpretation": "",
        "credentials_valid": False,
        "credentials_error": None,
        "credentials": {},
        "parallel_status": {},
        "error_states": {},
        "stream_events": [],
        "company_namespace": "amazon"  # Explicitly set namespace for testing
    }

    # Build and invoke graph
    logging.info("Building and invoking analysis graph...")
    graph = build_vectordb_retrieval_graph()
    
    try:
        final_state = graph.invoke(test_state)
        logging.info("\nAnalysis completed successfully.")
    except Exception as e:
        logging.error(f"Analysis failed with error: {str(e)}", exc_info=True)
        raise

    logging.info("\n===== Final Interpretation =====\n")
    logging.info(final_state["final_interpretation"])
    logging.info("\n====================================")
    logging.info("Test run completed.")

###############################################################################
# Utility Functions
###############################################################################
def parse_llm_json_response(response_text: str) -> dict:
    """
    Utility function to parse JSON from LLM responses, handling common formats
    like markdown code blocks.
    """
    text = response_text.strip()
    
    # Remove markdown code blocks if present
    if text.startswith('```json'):
        text = text[7:]
    elif text.startswith('```'):
        text = text[3:]
    if text.endswith('```'):
        text = text[:-3]
    
    # Parse and validate JSON
    try:
        return json.loads(text.strip())
    except json.JSONDecodeError as e:
        logging.error(f"Failed to parse JSON response: {e}")
        logging.debug(f"Raw response text: {text}")
        raise

---

<filepath>analysis_module_execution.py</filepath>

import os
import logging
from typing import TypedDict, Optional, List, Dict, Any, Literal, Union
import google.generativeai as genai
from langchain_core.runnables import RunnableConfig
from langgraph.graph.state import StateGraph, START, END
from langgraph.types import Command
from langfuse.decorators import observe

"""
EXAMPLE: Analysis Execution Sub-Graph

PURPOSE:
- Demonstrate how you might structure an "Execution" sub-graph (Module 4)
  that can do calculations, run text-based analysis, and format the output.

STATE DESIGN:
We define a typed dictionary 'AnalysisExecutionState' capturing:
- 'execution_plan':  The plan steps from the planning module,
                     including any references to information needs or data
- 'collected_data':  The dictionary or store that contains the factual/broad data retrieved
- 'calculation_results': Output from any numeric or code-based calculations
- 'analysis_text':   Output from text-based analysis steps
- 'formatted_output': Final user-ready response or structured JSON or whatever
"""

class AnalysisExecutionState(TypedDict):
    # The set of instructions or steps from the planning module
    execution_plan: List[Dict[str, Any]]
    
    # Data from the info-gathering module. Each 'stored' key referencing the final text or numbers
    collected_data: Dict[str, Any]

    # Results of numeric or code-based calculations
    calculation_results: Dict[str, Any]

    # Results from text-based analysis
    analysis_text: str

    # Final user-ready string
    formatted_output: str

###############################################################################
# Node 1: parse_plan_for_steps
###############################################################################
@observe()
def parse_plan_for_steps(state: AnalysisExecutionState, config: RunnableConfig) -> Any:
    """
    Reads the 'execution_plan' from the planning module and decides which steps
    to run. If there's a 'FinancialCalculation' step, we route to do_calculations;
    if 'TextAnalysis' step, route to text_analysis; possibly multiple steps in parallel.

    For simplicity, we handle them in a predetermined sequence. Real logic
    might read the plan or store step indices.
    """
    # If your plan is more dynamic, you can parse it here and create parallel sends.
    logging.info("Parsing execution_plan for steps...")

    # We'll just assume a fixed approach: do calculations -> text analysis -> output formatting
    return Command(
        update={},
        goto="do_calculations"
    )

###############################################################################
# Node 2: do_calculations
###############################################################################
@observe()
def do_calculations(state: AnalysisExecutionState, config: RunnableConfig) -> Any:
    """
    Example node for numeric or code-based calculations.
    Suppose we do ratio analysis from 'collected_data'.

    We'll store the results in 'calculation_results'.
    """
    logging.info("Performing calculations...")

    # In a real scenario, you might parse the plan or read from collected_data for raw figures
    # Example: computing some ratio
    # We'll just demonstrate storing a mock ratio calculation.

    mock_ratio = 42.7  # Hard-coded for demonstration
    new_calc_results = {"sample_ratio": mock_ratio}

    # Merge with existing if needed
    current_calc = state.get("calculation_results", {})
    current_calc.update(new_calc_results)
    return {
        "calculation_results": current_calc
    }

###############################################################################
# Node 3: text_analysis
###############################################################################
@observe()
def text_analysis(state: AnalysisExecutionState, config: RunnableConfig) -> Any:
    """
    Suppose we want to produce a textual analysis referencing the results from calculations
    and the 'collected_data' (like broad overview of risk factors or management analysis).
    We'll do an LLM call (Gemini) or any other text model.
    """
    logging.info("Running text-based analysis with an LLM...")

    # Gather relevant data from state
    calcs = state.get("calculation_results", {})
    collected_data = state.get("collected_data", {})

    # Build a prompt that uses the data
    # Use minimal approach here
    ratio_str = calcs.get("sample_ratio", "unknown ratio")
    broad_summary = collected_data.get("broad_overview", "No broad overview found")

    # If you have an actual Gemini key:
    gemini_api_key = os.getenv("GEMINI_API_KEY", "")
    if gemini_api_key:
        genai.configure(api_key=gemini_api_key)
        model = genai.GenerativeModel("gemini-1.5-flash")
        prompt = (
            "You are analyzing financial data. We have a sample ratio and some broad text.\n"
            f"Ratio: {ratio_str}\n"
            f"Overview: {broad_summary}\n"
            "Please provide a short textual analysis incorporating both.\n"
        )
        try:
            response = model.generate_content(
                contents=prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.2,
                    max_output_tokens=300
                )
            )
            analysis = response.text.strip()
        except Exception as e:
            logging.error(f"LLM call failed: {e}")
            analysis = (
                "Failed to call LLM. Provide fallback analysis. "
                f"Ratio={ratio_str}, Overview={broad_summary}"
            )
    else:
        # If no key, do a mock approach
        analysis = f"This is a mock analysis referencing ratio={ratio_str} and overview={broad_summary}."

    return {
        "analysis_text": analysis
    }

###############################################################################
# Node 4: result_formatting
###############################################################################
@observe()
def result_formatting(state: AnalysisExecutionState, config: RunnableConfig) -> AnalysisExecutionState:
    """
    Final node that takes the calculations, text-based analysis, and any other data
    to produce a final user-ready string or structured JSON.
    """
    logging.info("Formatting the final result...")

    calcs = state.get("calculation_results", {})
    analysis_text = state.get("analysis_text", "")
    # Example of building a final string
    final_str = (
        "=== Analysis Results ===\n"
        f"Calculated sample ratio: {calcs.get('sample_ratio', 'N/A')}\n\n"
        f"Textual Analysis:\n{analysis_text}\n"
        "========================\n"
    )
    state["formatted_output"] = final_str
    return state

###############################################################################
# Build the sub-graph
###############################################################################
def build_analysis_execution_graph() -> StateGraph:
    """
    Returns a compiled StateGraph representing a sub-graph that:
     1) parse_plan_for_steps
     2) do_calculations
     3) text_analysis
     4) result_formatting
    """
    builder = StateGraph(AnalysisExecutionState)

    builder.add_node("parse_plan_for_steps", parse_plan_for_steps)
    builder.add_node("do_calculations", do_calculations)
    builder.add_node("text_analysis", text_analysis)
    builder.add_node("result_formatting", result_formatting)

    # Edges
    builder.add_edge(START, "parse_plan_for_steps")
    builder.add_edge("parse_plan_for_steps", "do_calculations")
    builder.add_edge("do_calculations", "text_analysis")
    builder.add_edge("text_analysis", "result_formatting")
    builder.add_edge("result_formatting", END)

    return builder.compile()

###############################################################################
# Example usage
###############################################################################
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    graph = build_analysis_execution_graph()

    # Example initial state that includes an execution_plan, some collected_data, etc.
    initial_state: AnalysisExecutionState = {
        "execution_plan": [
            {"stepName": "Sample Step", "actions": [{"actionType": "FinancialCalculation"}]},
            {"stepName": "Sample Step 2", "actions": [{"actionType": "TextAnalysis"}]}
        ],
        "collected_data": {"broad_overview": "Company shows stable revenue growth."},
        "calculation_results": {},
        "analysis_text": "",
        "formatted_output": ""
    }

    final_state = graph.invoke(initial_state)
    print("\n=== Final Output ===\n")
    print(final_state["formatted_output"])

---
