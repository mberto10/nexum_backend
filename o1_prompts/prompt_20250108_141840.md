<filepath>prompt_builder.md</filepath>

<user_instructions>

This script will be part of a financial agent application and this particular script is for interpreting a user query and making a plan for the agent to follow.

The execution tool for the next step will be a vector database with parsed 10k financial documents. What would be improvements that would generate the best plan for the agent to follow?

Make some recommendations for improvements. 



</user_instructions>



---

<filepath>analysis_module_plan.py</filepath>

import os
import google.generativeai as genai
from typing import TypedDict, Optional, List, Dict, Any
from langgraph.graph.state import StateGraph, START, END
from langchain_core.runnables import RunnableConfig
from langgraph.types import Command
import json

# ----------------------------------------------------------------------------
# 1. State Definition
# ----------------------------------------------------------------------------

class AnalysisState(TypedDict):
    """
    Holds the entire state for the initial user input + planning module.
    """
    # Module 1: User Input
    user_query: str
    company_name: str

    # Planning Output Fields
    analysis_type: Optional[str]
    # Updated: We only store 'information_needs' with summary, description, tag, stored
    information_needs: Optional[List[Dict[str, str]]]
    execution_plan: Optional[List[Dict[str, Any]]]  # array of steps for final analysis/execution
    plan_notes: Optional[str]

    # Entire Raw Plan
    plan_raw: Optional[Dict[str, Any]]

# ----------------------------------------------------------------------------
# 2. Context Cache (Workflow + Modules Overview)
# ----------------------------------------------------------------------------

CONTEXT_CACHE = """
TASK:
You are building an Analysis Agent to handle user requests related to financial 10-K data.

INSTRUCTIONS:
- The agent's workflow is divided into modules:
  1) User Input
  2) Planning Module
  3) Information Gathering Module (handled separately)
  4) Analysis Execution Module

- The primary information source is a set of 10-K financial documents stored in a vector database (Pinecone).
  These documents are chunked, with each chunk tagged by metadata (e.g., top-level item headings, subheadings).
  The user might ask for factual data (e.g., total revenue) or broad analysis (e.g., discussion of risk factors).

- Your role here: Provide a structured plan in JSON to orchestrate both how to store necessary information
  and the steps for final analysis. Query formulation or advanced retrieval steps are not part of this JSON
  (they happen in the Information Gathering Module).

JSON OUTPUT REQUIRED FIELDS:
{
  "analysisType": "string",
  "informationNeeds": [
    {
      "summary": "e.g. Net Profit data",
      "description": "Details or format of how data should look, any notes for context",
      "tag": "factual" or "broad",
      "stored": "identifier to store the result of this data retrieval"
    }
  ],
  "executionPlan": [
    {
      "stepName": "Descriptive name of step",
      "actions": [
        {
          "actionType": "One of: FinancialCalculation, TextAnalysis, ComparisonAnalysis, ScenarioModeling, WebResearch, ResultFormatting",
          "task": "Clear prompt or instructions for how to proceed with that step",
          "notes": "Extra instructions or caveats if any",
          "information": ["references to one or more stored variables from the informationNeeds"]
        }
      ]
    }
  ],
  "notes": "Overall plan-level notes"
}

IMPORTANT POINTS:
1) Return only valid JSON (no markdown).
2) The last step in executionPlan must have 'actionType': 'ResultFormatting' describing how to present final results.
3) No references to code or retrieval queries â€“ just the plan structure.
"""

# ----------------------------------------------------------------------------
# 3. Configure Gemini & (Optional) Context Caching
# ----------------------------------------------------------------------------

API_KEY = os.environ.get("GEMINI_API_KEY")
genai.configure(api_key=API_KEY)

def create_context_cache():
    """
    Placeholder for advanced caching usage if you want to store large doc(s)
    in a context-based approach. Not implementing in detail here.
    """
    return None

def get_planning_model():
    model = genai.GenerativeModel(
        model_name="gemini-1.5-flash",
        generation_config=genai.types.GenerationConfig(
            temperature=0.2,
            max_output_tokens=2500,
        )
    )

    # Define the response schema for structured output
    model.response_schema = {
        "type": "object",
        "properties": {
            "analysisType": {"type": "string"},
            "informationNeeds": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "summary": {"type": "string"},
                        "description": {"type": "string"},
                        "tag": {
                            "type": "string",
                            "enum": ["factual", "broad"]
                        },
                        "stored": {"type": "string"}
                    },
                    "required": ["summary", "description", "tag", "stored"]
                }
            },
            "executionPlan": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "stepName": {"type": "string"},
                        "actions": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "actionType": {
                                        "type": "string",
                                        "enum": [
                                            "FinancialCalculation",
                                            "TextAnalysis",
                                            "ComparisonAnalysis",
                                            "ScenarioModeling",
                                            "WebResearch",
                                            "ResultFormatting"
                                        ]
                                    },
                                    "task": {"type": "string"},
                                    "notes": {"type": "string"},
                                    "information": {
                                        "type": "array",
                                        "items": {"type": "string"}
                                    }
                                },
                                "required": ["actionType", "task", "information"]
                            }
                        }
                    },
                    "required": ["stepName", "actions"]
                }
            },
            "notes": {"type": "string"}
        },
        "required": [
            "analysisType",
            "informationNeeds",
            "executionPlan",
            "notes"
        ]
    }
    
    return model

# ----------------------------------------------------------------------------
# 4. Planning Node
# ----------------------------------------------------------------------------

def planning_node(state: AnalysisState, config: RunnableConfig, use_context_cache: bool = True) -> AnalysisState:
    """
    Module 2 - Planning:
    1. Accept user_query & company_name from state
    2. Optionally provide the big context (CONTEXT_CACHE) to the LLM
    3. Call Gemini to produce a JSON-based plan
    4. Parse & store each portion into state
    """
    user_query = state.get("user_query", "")
    company_name = state.get("company_name", "")
    if not user_query or not company_name:
        state["plan_raw"] = {
            "error": "Missing user_query or company_name",
            "raw_response": None
        }
        return state

    # Build prompt
    context_part = CONTEXT_CACHE if use_context_cache else ""
    prompt = f"""
{context_part}

User's query: '{user_query}'
Company name: '{company_name}'

Return a JSON object following the schema provided. Do not include any markdown formatting or extra text.
"""

    model = get_planning_model()

    raw_text = ""
    try:
        response = model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=0.0,
                max_output_tokens=1500,
            )
        )
        raw_text = response.text.strip()
        
        # Remove any markdown formatting if present
        if raw_text.startswith("```"):
            raw_text = raw_text.split("```")[1]
            # If there's a "json" fence, remove it
            if raw_text.startswith("json"):
                raw_text = raw_text[4:]
        raw_text = raw_text.strip()
        
        # Parse JSON
        parsed = json.loads(raw_text)
        
        # Store the full plan in plan_raw
        state["plan_raw"] = parsed

        # Distribute fields into separate state variables
        state["analysis_type"] = parsed.get("analysisType")
        state["information_needs"] = parsed.get("informationNeeds")
        state["execution_plan"] = parsed.get("executionPlan")
        state["plan_notes"] = parsed.get("notes")

    except Exception as e:
        state["plan_raw"] = {
            "error": f"Error processing response: {str(e)}",
            "raw_response": raw_text
        }

    return state

# ----------------------------------------------------------------------------
# 5. Graph Setup
# ----------------------------------------------------------------------------

def build_initial_planning_graph(use_context_cache: bool = True):
    """
    Builds a simple LangGraph workflow:
      START -> planning_node -> END
    """
    builder = StateGraph(AnalysisState)

    def planning_node_wrapper(state: AnalysisState, config: RunnableConfig) -> AnalysisState:
        return planning_node(state, config, use_context_cache=use_context_cache)

    builder.add_node("planning_node", planning_node_wrapper)

    builder.add_edge(START, "planning_node")
    builder.add_edge("planning_node", END)

    return builder.compile()

def run_planning_workflow(user_query: str, company_name: str, use_context_cache: bool = True) -> dict:
    """
    High-level function that sets up the graph state with user inputs,
    then runs the planning node.
    """
    graph = build_initial_planning_graph(use_context_cache=use_context_cache)
    initial_state: AnalysisState = {
        "user_query": user_query,
        "company_name": company_name,
        "analysis_type": None,
        "information_needs": None,
        "execution_plan": None,
        "plan_notes": None,
        "plan_raw": None
    }
    final_state = graph.invoke(initial_state)
    return final_state

# ----------------------------------------------------------------------------
# 6. Testing / Main
# ----------------------------------------------------------------------------

if __name__ == "__main__":
    test_query = "Analyze Amazon's main revenue streams and future growth potential."
    test_company = "Amazon"
    result = run_planning_workflow(test_query, test_company, use_context_cache=True)
    print("\n===== Analysis Planning Output =====")
    print(f"\nQuery: {test_query}")
    print(f"Company: {test_company}\n")
    
    plan = result.get("plan_raw", {})
    formatted_json = json.dumps(plan, indent=2)
    print(formatted_json)

---
